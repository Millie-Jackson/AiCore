{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 1 of '__getURL'\n",
      "Call 2 of '__getURL'\n",
      "__makeRaw_DataFolder Error\n",
      "raw_data Folder Already Exists\n",
      "Call 3 of '__getURL'\n",
      "__makeRaw_DataFolder Error\n",
      "raw_data Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "__makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "__makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "Call 4 of '__getURL'\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=103.0.5060.114)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00D26463+2188387]\n\tOrdinal0 [0x00CBE461+1762401]\n\tOrdinal0 [0x00BD3D78+802168]\n\tOrdinal0 [0x00BBC680+706176]\n\tOrdinal0 [0x00C1F169+1110377]\n\tOrdinal0 [0x00C2C3C2+1164226]\n\tOrdinal0 [0x00C1C5F6+1099254]\n\tOrdinal0 [0x00BF6BE0+945120]\n\tOrdinal0 [0x00BF7AD6+948950]\n\tGetHandleVerifier [0x00FC71F2+2712546]\n\tGetHandleVerifier [0x00FB886D+2652765]\n\tGetHandleVerifier [0x00DB002A+520730]\n\tGetHandleVerifier [0x00DAEE06+516086]\n\tOrdinal0 [0x00CC468B+1787531]\n\tOrdinal0 [0x00CC8E88+1805960]\n\tOrdinal0 [0x00CC8F75+1806197]\n\tOrdinal0 [0x00CD1DF1+1842673]\n\tBaseThreadInitThunk [0x75BEFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77C07A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77C07A6E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 410>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=406'>407</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__downloadImage(\u001b[39mself\u001b[39m, j, data\u001b[39m.\u001b[39mrecipeName)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=407'>408</a>\u001b[0m                 data\u001b[39m.\u001b[39mcount \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcount \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m                \n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=409'>410</a>\u001b[0m scraper\u001b[39m.\u001b[39;49mintitialize(scraper, \u001b[39m'\u001b[39;49m\u001b[39mhttps://www.pickuplimes.com\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlemons\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.intitialize\u001b[1;34m(self, url, searchTerm, delay)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=76'>77</a>\u001b[0m global_ids \u001b[39m=\u001b[39m scraper\u001b[39m.\u001b[39m__getUniqueID(scraper, \u001b[39m'\u001b[39m\u001b[39mhttps://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=78'>79</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getURL(url) \u001b[39m# Have to start somewhere\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=79'>80</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__run(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=80'>81</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__closeSession()\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.__run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=85'>86</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getAllRecipePages(\u001b[39mself\u001b[39m, data\u001b[39m.\u001b[39mcurrentURL)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=86'>87</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getRecipes(\u001b[39mself\u001b[39m, data\u001b[39m.\u001b[39mcurrentURL)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=87'>88</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__cycleRecipeLinks(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=88'>89</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__closeSession()\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.__cycleRecipeLinks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=91'>92</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mrecipeLinks:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=92'>93</a>\u001b[0m     data\u001b[39m.\u001b[39mcurrentURL \u001b[39m=\u001b[39m i\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=93'>94</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__makeImage(\u001b[39mself\u001b[39;49m, data\u001b[39m.\u001b[39;49mcurrentURL)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.__makeImage\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=390'>391</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__makeImage\u001b[39m(\u001b[39mself\u001b[39m, url):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=391'>392</a>\u001b[0m     \u001b[39m'''Retrieves the ID of each image using 'getRecipeDetails()\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=392'>393</a>\u001b[0m \u001b[39m    Removes all unecissary elements from the ID string to create a file name\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=393'>394</a>\u001b[0m \u001b[39m    Pass the file name to 'downloadImages() to create a file'''\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=395'>396</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__getRecipeDetails(\u001b[39mself\u001b[39;49m, url)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=397'>398</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mrecipeDetails[\u001b[39m'\u001b[39m\u001b[39mImages\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=398'>399</a>\u001b[0m         \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m i:\n",
      "File \u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\decorators.py:11\u001b[0m, in \u001b[0;36mexceptionHandling.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func) \u001b[39m# maintains introspection\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     10\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     12\u001b[0m     \u001b[39mexcept\u001b[39;00m NoSuchElementException:\n\u001b[0;32m     13\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Exception: Element Not Found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.__getRecipeDetails\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=278'>279</a>\u001b[0m \u001b[39m@decorators\u001b[39m\u001b[39m.\u001b[39mexceptionHandling\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=279'>280</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getRecipeDetails\u001b[39m(\u001b[39mself\u001b[39m, url):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=280'>281</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getURL(url)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=282'>283</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__scrapeName()  \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=283'>284</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__scrapeTags()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=284'>285</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__scrapeDescription()\n",
      "File \u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\decorators.py:25\u001b[0m, in \u001b[0;36mscrapeHandling.<locals>.wrapperOuter.<locals>.wrapperInner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func) \u001b[39m# maintains introspection\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapperInner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m NoSuchElementException:\n\u001b[0;32m     27\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Exception:\u001b[39m\u001b[39m\"\u001b[39m, element, \u001b[39m\"\u001b[39m\u001b[39mNot Found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.__scrapeName\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=198'>199</a>\u001b[0m \u001b[39m@decorators\u001b[39m\u001b[39m.\u001b[39mscrapeHandling(data\u001b[39m.\u001b[39mname)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=199'>200</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__scrapeName\u001b[39m():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=201'>202</a>\u001b[0m     data\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m WebDriverWait(driver, \u001b[39m5\u001b[39;49m)\u001b[39m.\u001b[39;49muntil(EC\u001b[39m.\u001b[39;49mpresence_of_element_located((By\u001b[39m.\u001b[39;49mXPATH, \u001b[39m'\u001b[39;49m\u001b[39m//*[@id=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mheader-info-col\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m]/div/header/h1\u001b[39;49m\u001b[39m'\u001b[39;49m)))\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:78\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m         value \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_driver)\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m value:\n\u001b[0;32m     80\u001b[0m             \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:64\u001b[0m, in \u001b[0;36mpresence_of_element_located.<locals>._predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predicate\u001b[39m(driver):\n\u001b[1;32m---> 64\u001b[0m     \u001b[39mreturn\u001b[39;00m driver\u001b[39m.\u001b[39;49mfind_element(\u001b[39m*\u001b[39;49mlocator)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:1248\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m   1246\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m value\n\u001b[1;32m-> 1248\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\n\u001b[0;32m   1249\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m'\u001b[39;49m: by,\n\u001b[0;32m   1250\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m'\u001b[39;49m: value})[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:425\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    423\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    424\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 425\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    426\u001b[0m     response[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(\n\u001b[0;32m    427\u001b[0m         response\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    428\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:247\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m'\u001b[39m\u001b[39malert\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    246\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=103.0.5060.114)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00D26463+2188387]\n\tOrdinal0 [0x00CBE461+1762401]\n\tOrdinal0 [0x00BD3D78+802168]\n\tOrdinal0 [0x00BBC680+706176]\n\tOrdinal0 [0x00C1F169+1110377]\n\tOrdinal0 [0x00C2C3C2+1164226]\n\tOrdinal0 [0x00C1C5F6+1099254]\n\tOrdinal0 [0x00BF6BE0+945120]\n\tOrdinal0 [0x00BF7AD6+948950]\n\tGetHandleVerifier [0x00FC71F2+2712546]\n\tGetHandleVerifier [0x00FB886D+2652765]\n\tGetHandleVerifier [0x00DB002A+520730]\n\tGetHandleVerifier [0x00DAEE06+516086]\n\tOrdinal0 [0x00CC468B+1787531]\n\tOrdinal0 [0x00CC8E88+1805960]\n\tOrdinal0 [0x00CC8F75+1806197]\n\tOrdinal0 [0x00CD1DF1+1842673]\n\tBaseThreadInitThunk [0x75BEFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77C07A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77C07A6E+238]\n"
     ]
    }
   ],
   "source": [
    "import functools # used to maintain introspection on decorators\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "import uuid # used to create a unique 'computer' id for each recipe\n",
    "import json # used to store the scraped details\n",
    "import os\n",
    "\n",
    "from uuid import UUID # used to create a unique id for each recipe\n",
    "from json import JSONEncoder # used to convert the UUID into a writable format\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "#import decorators\n",
    "from decorators import callCount\n",
    "from decorators import exceptionHandling # used for genral exception handling\n",
    "from decorators import scrapeHandling # used for scraping specific exception handling\n",
    "from decorators import folderAlreadyExists # used for folder creation\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "class data:\n",
    "\n",
    "    articles = [] # Used to make a list of recipes\n",
    "    button = None # Used to interact with various button elements\n",
    "    container = None # Used to store various container elements\n",
    "    currentURL = \"\" # Used to store various urls \n",
    "    pages = [] # Used to append a list with pages links\n",
    "    recipeLinks = [] # Used to store recipe links\n",
    "    recipeName = \"\" # Stores the recipe name\n",
    "    searchbar = None # Used to interact with search bar\n",
    "    source = \"\" # Used to get page source code\n",
    "    tag = None # Used to store various tag elements\n",
    "    title = \"\" # Used to get the title\n",
    "    totalPages = [] # Stores a list of pages\n",
    "\n",
    "    # File Management\n",
    "    count = 0 # Used in the creation of image filenames\n",
    "    dataDirectory = \"\" # Used to create folder\n",
    "    imageDirectory = \"\" # Used to create folder \n",
    "    recipeDirectory = \"\" # Used to create modified folder names\n",
    "\n",
    "    # Scraped Information\n",
    "    recipeDetails = {} # Used to store all the scraped recipe details\n",
    "\n",
    "    allergens = \"\" # Used to store scraped allergens\n",
    "    alternatives = \"\" #Used to store scraped alternatives\n",
    "    description = \"\" # Used to store the scraped description of the recipe\n",
    "    freeFrom = \"\" # Used to store the scraped free from information\n",
    "    imageLinks = [] # Used to scrape all of a recipes image links\n",
    "    ingredients = \"\" # Used to store the scraped ingredients\n",
    "    instructions = \"\" # Used to store scraped instructions\n",
    "    mainPhoto = None # Used to store main photo link\n",
    "    name = \"\" # Used to store scraped recipe name\n",
    "    notes = \"\" # Used to store scraped recipe notes\n",
    "    recipeTags = \"\" # Used to store scraped recipe tags\n",
    "    storage = \"\" # Used to store scraped storage instructions\n",
    "    timeCook = \"\" # Used to store scraped cook time\n",
    "    timePrep = \"\" # Used to store scraped recipe  prep time \n",
    "    timeTotal = \"\" # Used to store scraped total time it takes to make the recipe\n",
    "\n",
    "class scraper:\n",
    "    def intitialize(self, url, searchTerm, delay):\n",
    "        global_ids = scraper.__getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "    \n",
    "        self.__getURL(url) # Have to start somewhere\n",
    "        self.__run(self)\n",
    "        self.__closeSession() # Have to end somewhere\n",
    "\n",
    "    def __run(self):\n",
    "        #self.__acceptCookies()\n",
    "        data.currentURL = self.__findRecipeList(self)\n",
    "        self.__getAllRecipePages(self, data.currentURL)\n",
    "        self.__getRecipes(self, data.currentURL)\n",
    "        self.__cycleRecipeLinks(self)\n",
    "        self.__closeSession()   \n",
    "\n",
    "    def __cycleRecipeLinks(self):\n",
    "        for i in data.recipeLinks:\n",
    "            data.currentURL = i\n",
    "            self.__makeImage(self, data.currentURL)\n",
    "\n",
    "    @callCount\n",
    "    def __getURL(url):\n",
    "        '''Navigates to a website using a url passed as a perameter.'''\n",
    "        driver.get(url) \n",
    "\n",
    "    def __getTitle():\n",
    "        '''Fetches the title and prints it to screen.'''\n",
    "        data.title = driver.title\n",
    "\n",
    "    def __closeSession():\n",
    "        '''Closes the driver'''\n",
    "        driver.quit()\n",
    "\n",
    "    def __getSourceCode():\n",
    "        '''Fetches source code for the page.'''\n",
    "        data.source = driver.page_source\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __search(self, searchTerm):\n",
    "        '''Finds search bar, types in the search term which it takes as a perameter and clicks to navigate to the next page.'''\n",
    "        button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'nav-searchbar-btn')))\n",
    "        button.click()\n",
    "\n",
    "        self.findSearchbar(self, searchTerm)\n",
    "\n",
    "    def __searchbarTextAndClick(searchTerm):\n",
    "        try:\n",
    "            data.searchbar.send_keys(searchTerm)\n",
    "            data.searchbar.send_keys(Keys.RETURN) # Return = Enter\n",
    "        except:\n",
    "            print(\"Exception: No search term input\")\n",
    "    \n",
    "    @decorators.exceptionHandling\n",
    "    def __findSearchbar(self, searchTerm):\n",
    "\n",
    "        data.searchbar = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.NAME, \"sb\")))\n",
    "        self.__searchbarTextAndClick(searchTerm)\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __home():\n",
    "        '''Finds the title and clicks it.'''\n",
    "        title = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'nav-image')))\n",
    "        title.click()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __findRecipeList(self):\n",
    "        '''Finds the recipe tab and clicks it.'''\n",
    "        data.button = WebDriverWait(driver,5).until(EC.presence_of_element_located((By.LINK_TEXT, 'Recipes')))\n",
    "        data.button.click()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __acceptCookies():\n",
    "        '''Finds the accept cookies button and clicks it.'''\n",
    "        data.button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/div[2]/div[2]')))\n",
    "        data.button.click()\n",
    "    \n",
    "    def __getRecipes(self, url):\n",
    "        '''Finds the recipe container and puts all the recipes in a list.'''\n",
    "\n",
    "        self.__getRecipeContainer()\n",
    "        self.__makeRecipeList()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __getRecipeContainer():\n",
    "        data.container = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='index-item-container']/div/div[2]/ul\"))) \n",
    "\n",
    "    def __makeRecipeList():\n",
    "        data.articles = data.container.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "        for i in data.articles:\n",
    "            data.tag = i.find_element(By.TAG_NAME, 'a')\n",
    "            data.recipeLinks.append(data.tag.get_attribute('href'))\n",
    "\n",
    "    def __getPageURL():\n",
    "        '''Returns the current page url.'''\n",
    "        data.currentURL =  driver.current_url\n",
    "\n",
    "    def __getAllRecipePages(self, url):\n",
    "        '''Navigates to each recipe page by modifying the current url and stores them in a list.'''\n",
    "\n",
    "        self.__getTotalPages()\n",
    "        self.__getSearchList()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __getTotalPages():\n",
    "         #totalPages = driver.find_element(By.CLASS_NAME, 'page-text') #actual\n",
    "        data.totalPages = [1, 2, 3] #temp to shorten runtime\n",
    "\n",
    "    def __getSearchList():\n",
    "        for i in data.totalPages:\n",
    "            data.currentURL = driver.current_url\n",
    "            url_change = \"?page=\" + str(i)\n",
    "            next_page = data.currentURL + url_change\n",
    "            data.pages.append(next_page)\n",
    "\n",
    "    def __getUniqueID(self, url):\n",
    "        '''Creates a uuid for each recipe taking a url as a perameter'''\n",
    "        \n",
    "        page_ID = url\n",
    "        just_ID = page_ID.replace(str(\"https://www.pickuplimes.com/recipe/\"), \"\")\n",
    "\n",
    "        ids = (just_ID, uuid.uuid4())\n",
    "\n",
    "    @decorators.scrapeHandling(data.name)\n",
    "    def __scrapeName():\n",
    "\n",
    "        data.name = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/h1'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.recipeTags)\n",
    "    def __scrapeTags():\n",
    "\n",
    "        data.recipeTags = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"header-info-col\"]/div/header/a[1]/div/p'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.description)\n",
    "    def __scrapeDescription():\n",
    "\n",
    "        data.description = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/span'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.timeTotal)\n",
    "    def __scrapeTotalTime():\n",
    "        \n",
    "        data.timeTotal = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[2]'))).text  \n",
    "\n",
    "    @decorators.scrapeHandling(data.timePrep)\n",
    "    def __scrapePrepTime():\n",
    "\n",
    "        data.timePrep = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[3]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.timeCook)\n",
    "    def __scrapeCookTime():\n",
    "        \n",
    "        data.timeCook = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[4]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.allergens)\n",
    "    def __scrapeAllergens():\n",
    "\n",
    "        data.allergens = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[1]/div'))).text \n",
    "\n",
    "    @decorators.scrapeHandling(data.alternatives)\n",
    "    def __scrapeAlternatives():\n",
    "        \n",
    "        data.alternatives = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[2]/div'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.freeFrom)\n",
    "    def __scrapeFreeFrom():\n",
    "        \n",
    "        data.freeFrom = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[3]/div'))).text \n",
    "\n",
    "    @decorators.scrapeHandling(data.ingredients)\n",
    "    def __scrapeIngredients():\n",
    "        \n",
    "        data.ingredients = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[2]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.instructions)\n",
    "    def __scrapeInstructions():\n",
    "        \n",
    "        data.instructions = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ol'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.notes)\n",
    "    def __scrapeNotes():\n",
    "        \n",
    "        data.notes = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[1]/li'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.storage)\n",
    "    def __scrapeStorage():\n",
    "        \n",
    "        data.storage = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[2]/li'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.mainPhoto)\n",
    "    def __scrapeMainPhoto():\n",
    "        \n",
    "        data.mainPhoto = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main-image-container\"]/img')))\n",
    "\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __scrapeImages():\n",
    "        imageContainer = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-video\"]/div[2]'))) # Find the container\n",
    "        imageList = imageContainer.find_elements(By.XPATH, 'img') # Find the children\n",
    "\n",
    "        for i in imageList:\n",
    "            link = i.get_attribute('src')\n",
    "            data.imageLinks.append(link)\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __getRecipeDetails(self, url):\n",
    "        self.__getURL(url)\n",
    "\n",
    "        self.__scrapeName()  \n",
    "        self.__scrapeTags()\n",
    "        self.__scrapeDescription()\n",
    "        self.__scrapeTotalTime()\n",
    "        self.__scrapePrepTime()\n",
    "        self.__scrapeCookTime()\n",
    "        self.__scrapeAllergens()\n",
    "        self.__scrapeAlternatives()\n",
    "        self.__scrapeFreeFrom()\n",
    "        self.__scrapeIngredients()\n",
    "        self.__scrapeInstructions()\n",
    "        self.__scrapeNotes()\n",
    "        self.__scrapeStorage()\n",
    "        self.__scrapeMainPhoto()\n",
    "        self.__scrapeImages()\n",
    "\n",
    "        self.__storeDetails(self, url)\n",
    "        self.__jsonFile(self)\n",
    "\n",
    "    def __storeDetails(self, url):\n",
    "        data.recipeDetails = {'ID': [], 'Name': [], 'Photo': [],'Tags': [], 'Description': [], 'Total Time': [], 'Prep Time': [], 'Cook Time': [], 'Allergens': [], 'Swaps': [], 'Free From': [], 'Ingredients': [], 'Directions': [], 'Notes': [], 'Storage': [], 'Images': []}\n",
    "        data.recipeDetails['ID'].append(self.__getUniqueID(self, url))\n",
    "        data.recipeDetails['Name'].append(data.name)\n",
    "        data.recipeDetails['Photo'].append(data.mainPhoto)\n",
    "        data.recipeDetails['Tags'].append(data.recipeTags)\n",
    "        data.recipeDetails['Description'].append(data.description)\n",
    "        data.recipeDetails['Total Time'].append(data.timeTotal)\n",
    "        data.recipeDetails['Prep Time'].append(data.timePrep)\n",
    "        data.recipeDetails['Cook Time'].append(data.timeCook)\n",
    "        data.recipeDetails['Allergens'].append(data.allergens)\n",
    "        data.recipeDetails['Swaps'].append(data.alternatives)\n",
    "        data.recipeDetails['Free From'].append(data.freeFrom)\n",
    "        data.recipeDetails['Ingredients'].append(data.ingredients)\n",
    "        data.recipeDetails['Directions'].append(data.instructions)\n",
    "        data.recipeDetails['Notes'].append(data.notes)\n",
    "        data.recipeDetails['Storage'].append(data.storage)\n",
    "        data.recipeDetails['Images'].append(data.imageLinks)\n",
    "\n",
    "    def __jsonFile(self):\n",
    "        '''Creates a folder called 'raw_data' in the path for the json file to be saved in\n",
    "        Uses a try except catch as it will throw an error if the folder already exists'''\n",
    "\n",
    "        self.__makeRaw_DataFolder()\n",
    "\n",
    "        '''Deals with TypeError: Object of type UUID is not JSON serializable by encoding the UUID'''\n",
    "        JSONEncoder_olddefault = JSONEncoder.default\n",
    "        def __JSONEncoder_newdefault(self, o):\n",
    "            if isinstance(o, UUID): return str(o)\n",
    "            return JSONEncoder_olddefault(self, o)\n",
    "        JSONEncoder.default = __JSONEncoder_newdefault\n",
    "\n",
    "        self.__jsonDump()\n",
    "\n",
    "    @decorators.folderAlreadyExists(\"raw_data\")\n",
    "    def __makeRaw_DataFolder():\n",
    "        \n",
    "        data.dataDirectory = \"raw_data\"\n",
    "        parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "        path = os.path.join(parent_dir, data.dataDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.dataDirectory)\n",
    "    \n",
    "    def __jsonDump():\n",
    "        '''Stores data by writing the 'recipe_details' dictionary to a JSON file called 'data.json' in the folder just created\n",
    "        The dicrionary is converted to a string using str() to deal with 'TypeError: Object of type WebElement is not JSON serializable'''\n",
    "        with open(os.path.join('raw_data', 'data.json'), 'w') as json_file:\n",
    "            json.dump(str(data.recipeDetails), json_file)\n",
    "\n",
    "    def __downloadImage(self, url, recipeName):\n",
    "        '''Creates a folder called 'images' and another with the recipe name in the path for the image files to be saved in\n",
    "        Uses a try except catch as it will throw an error if the folders already exists\n",
    "        Adds User-Agent Headers to bypass 403 error\n",
    "        Downloads the images into the folder of that recipe name'''\n",
    "\n",
    "        self.__makeImagesFolder()\n",
    "        self.__makeRecipeFolder()\n",
    "        \n",
    "        try:\n",
    "            # Adds headers to resolve 403 Fobidden Error\n",
    "            opener=urllib.request.build_opener()\n",
    "            opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "            urllib.request.install_opener(opener)\n",
    "\n",
    "            downloadDirectory = \"images/\" + data.recipeDirectory + \"/\"\n",
    "            fileType = '.jpg'\n",
    "            fileName = downloadDirectory + recipeName + fileType\n",
    "            image = urllib.request.urlretrieve(url, fileName)\n",
    "        except:\n",
    "            print(\"Error Downloading Images\")           \n",
    "\n",
    "    @decorators.folderAlreadyExists(\"Images\")\n",
    "    def __makeImagesFolder():\n",
    "        \n",
    "        data.imageDirectory = \"images\"\n",
    "        parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "        path = os.path.join(parent_dir, data.imageDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.imageDirectory)\n",
    "\n",
    "    @decorators.folderAlreadyExists(\"Recipe\")\n",
    "    def __makeRecipeFolder():\n",
    "\n",
    "        data.recipeDirectory = data.recipeName.replace(\".jpg\", \"\").replace(\"0\", \"\").replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\").replace(\"8\", \"\").replace(\"9\", \"\")\n",
    "        parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/images\"\n",
    "        path = os.path.join(parent_dir, data.recipeDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.recipeDirectory)\n",
    "\n",
    "    def __makeImage(self, url):\n",
    "        '''Retrieves the ID of each image using 'getRecipeDetails()\n",
    "        Removes all unecissary elements from the ID string to create a file name\n",
    "        Pass the file name to 'downloadImages() to create a file'''\n",
    "        \n",
    "        self.__getRecipeDetails(self, url)\n",
    "        \n",
    "        for i in data.recipeDetails['Images']:\n",
    "            for j in i:\n",
    "                IDtoName = str(data.recipeDetails['ID']).split()\n",
    "                IDtoName1 = str(IDtoName[0]).replace(\"(\", \"\")\n",
    "                IDtoName2 = str(IDtoName1).replace(\"[\", \"\")\n",
    "                IDtoName3 = str(IDtoName2).replace(\",\", \"\")\n",
    "                IDtoName4 = str(IDtoName3).replace(\"'\", \"\")\n",
    "\n",
    "                data.recipeName = IDtoName4 + \"-\" + str(data.count) + \".jpg\"\n",
    "                self.__downloadImage(self, j, data.recipeName)\n",
    "                data.count = data.count + 1                \n",
    "\n",
    "scraper.intitialize(scraper, 'https://www.pickuplimes.com', 'lemons', 5)\n",
    "\n",
    "\n",
    "# IDEALS ---------------------------------------------------------------------------------------------------------------\n",
    "# link getTitle() and home()\n",
    "# link getPageURL() and getUniqueID()\n",
    "# get search results\n",
    "# get search results on more than the first page\n",
    "# make a separate main image\n",
    "# counts how many recipes there are on the site\n",
    "# a for loop that automaticaly changes the xpath to get all the images for the recipe\n",
    "# bypass login\n",
    "# replace all XPaths with written XPaths\n",
    "# a method that makes a list containing all buttons and cycles through them looking for a link_text given as an argument\n",
    "# seperate website specific methods from general functioning in a class\n",
    "# make sure all code works with any website so is reusable and genralisable\n",
    "# use the unique id (page url) to stop scraping recipes it has already scraped \n",
    "# have a wait for element function that takes a xpath and a perameter\n",
    "# time each function to optimise run speed\n",
    "# error handeling function? all error hadling is done by one function?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89f331584aba9fd139438438d44e9f0981e5b7ec53d275c2a7e5fdd51f9e359f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataCollectionPipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
