{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Root Folder Already Exists\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "\n",
    "from json import JSONEncoder\n",
    "from uuid import UUID\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "class scraper:\n",
    "    def intitialize(self, url, search_term, delay):\n",
    "        global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "        #seconds = delay\n",
    "        \n",
    "        self.getURL(url)\n",
    "        #self.getTitle()\n",
    "        #self.acceptCookies()\n",
    "        #self.getAllRecipePages()\n",
    "        #self.getSourceCode()\n",
    "        #self.search(search_term)\n",
    "        #time.sleep(3)\n",
    "        #self.home()\n",
    "        #self.findRecipeList()\n",
    "        #self.getRecipes()\n",
    "        #self.getPageURL()\n",
    "        #self.getUniqueID()\n",
    "        self.getRecipeDetails(self)\n",
    "        #time.sleep(3)\n",
    "\n",
    "        self.closeSession()\n",
    "\n",
    "    def getURL(url):\n",
    "        driver.get(url) \n",
    "\n",
    "    def getTitle():\n",
    "        print(driver.title)\n",
    "\n",
    "    def closeSession():\n",
    "        time.sleep(3)\n",
    "        driver.quit()\n",
    "\n",
    "    def getSourceCode():\n",
    "        print(driver.page_source)\n",
    "\n",
    "    def search(search_term):\n",
    "        try:\n",
    "            button = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.ID, 'nav-searchbar-btn')))\n",
    "            button.click()\n",
    "            try:\n",
    "                search_bar = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.NAME, \"sb\")))\n",
    "                try:\n",
    "                    search_bar.send_keys(search_term)\n",
    "                    search_bar.send_keys(Keys.RETURN) # Return = Enter\n",
    "                except:\n",
    "                    print(\"Exception: No search term input\")\n",
    "            except NoSuchElementException:\n",
    "                print(\"Exception: No search bar found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Search bar\")\n",
    "\n",
    "    def home():\n",
    "        try:\n",
    "            title = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.ID, 'nav-image')))\n",
    "            title.click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: Title Not Found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Title\")\n",
    "\n",
    "    def findRecipeList():\n",
    "        try:\n",
    "            button = WebDriverWait(driver,seconds).until(EC.presence_of_element_located((By.LINK_TEXT, 'Recipes')))\n",
    "            button.click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: Recipe List Not Found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Recipe List\")\n",
    "\n",
    "    def acceptCookies():\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/div[2]/div[2]')))\n",
    "            cookie_button.click()\n",
    "            print(\"Removed Cookies\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exeption: Didnt Find Cookie Button\")\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout: Accept Cookie\")\n",
    "    \n",
    "    def getRecipes():\n",
    "        try:\n",
    "            main = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.ID, 'index-item-container')))      \n",
    "            articles = []\n",
    "            articles = main.find_elements(By.TAG_NAME, 'li')\n",
    "            print(\"Number of Recipes:\", len(articles))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: Can't Find Recipe List\")\n",
    "        except TimeoutException: \n",
    "            print(\"Exception: Timeout: Can't Find Recipe List\")\n",
    "\n",
    "        for i in articles:\n",
    "            print(\"Recipe:\" , i.text)\n",
    "\n",
    "    def getPageURL():\n",
    "        print(\"URL:\", driver.current_url)\n",
    "\n",
    "    def getAllRecipePages(self):\n",
    "        pages = []\n",
    "        self.getURL('https://www.pickuplimes.com/')\n",
    "        scraper.findRecipeList()\n",
    "        page = [driver.current_url]\n",
    "\n",
    "        try:\n",
    "            #total_pages = driver.find_element(By.CLASS_NAME, 'page-text') #actual\n",
    "            total_pages = [1, 2, 3, 4, 5] #temp to shorten runtime\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: Total Pages Not Found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Couldn't Find Total Pages\")\n",
    "\n",
    "        for i in total_pages:\n",
    "            current_page = driver.current_url\n",
    "            url_change = \"?page=\" + str(i)\n",
    "            next_page = current_page + url_change\n",
    "            print(next_page)\n",
    "            pages.append(next_page)\n",
    "            print(\"Number of Pages:\", len(pages))\n",
    "\n",
    "    def getUniqueID(self, url):\n",
    "        \n",
    "        page_ID = url\n",
    "        just_ID = page_ID.replace(str(\"https://www.pickuplimes.com/recipe/\"), \"\")\n",
    "\n",
    "        ids = (just_ID, uuid.uuid4())\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def getRecipeDetails(self):\n",
    "        self.getURL('https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "\n",
    "        try:\n",
    "            name = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/h1'))).text\n",
    "            tag = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"header-info-col\"]/div/header/a[1]/div/p'))).text\n",
    "            description = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/span'))).text\n",
    "            time_total = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[2]'))).text  \n",
    "            time_prep = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[3]'))).text\n",
    "            time_cook = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[4]'))).text\n",
    "            allergens = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[1]/div'))).text \n",
    "            swap = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[2]/div'))).text\n",
    "            free_from = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[3]/div'))).text  \n",
    "            ingredients = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[2]'))).text\n",
    "            directions = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ol'))).text\n",
    "            notes = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[1]/li'))).text\n",
    "            storage = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[2]/li'))).text\n",
    "            picture_main = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main-image-container\"]/img')))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: One Or More Data Entry Not Found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Didnt Find All Data Entries\")\n",
    "\n",
    "        image_container = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-video\"]/div[2]'))) # Find the container\n",
    "        image_list = image_container.find_elements(By.XPATH, 'img') # Find the children\n",
    "        print(len(image_list))\n",
    "        image_links= []\n",
    "\n",
    "        for i in image_list:\n",
    "            link = i.get_attribute('src')\n",
    "            image_links.append(link)\n",
    "        \n",
    "        self.recipe_details = {'ID': [], 'Name': [], 'Photo': [],'Tags': [], 'Description': [], 'Total Time': [], 'Prep Time': [], 'Cook Time': [], 'Allergens': [], 'Swaps': [], 'Free From': [], 'Ingredients': [], 'Directions': [], 'Notes': [], 'Storage': [], 'Images': []}\n",
    "        self.recipe_details['ID'].append(self.getUniqueID(self, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213'))\n",
    "        self.recipe_details['Name'].append(name)\n",
    "        self.recipe_details['Photo'].append(picture_main)\n",
    "        self.recipe_details['Tags'].append(tag)\n",
    "        self.recipe_details['Description'].append(description)\n",
    "        self.recipe_details['Total Time'].append(time_total)\n",
    "        self.recipe_details['Prep Time'].append(time_prep)\n",
    "        self.recipe_details['Cook Time'].append(time_cook)\n",
    "        self.recipe_details['Allergens'].append(allergens)\n",
    "        self.recipe_details['Swaps'].append(swap)\n",
    "        self.recipe_details['Free From'].append(free_from)\n",
    "        self.recipe_details['Ingredients'].append(ingredients)\n",
    "        self.recipe_details['Directions'].append(directions)\n",
    "        self.recipe_details['Notes'].append(notes)\n",
    "        self.recipe_details['Storage'].append(storage)\n",
    "        self.recipe_details['Images'].append(image_list)\n",
    "\n",
    "        self.jsonFile(self)\n",
    "\n",
    "    '''This function creates a folder for all the data then creates a json file and writes the data dictionary to it'''\n",
    "    def jsonFile(self):\n",
    "        '''Creates a folder called 'raw_data' in the path for the json file to be saved in\n",
    "        Uses a try except catch as it will throw an error if the folder already exists'''\n",
    "        try:\n",
    "            directory = \"raw_data\"\n",
    "            parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "            path = os.path.join(parent_dir, directory)\n",
    "            os.mkdir(path)\n",
    "            print(\"Directory '% s' created\" % directory)\n",
    "        except:\n",
    "            print(\"Root Folder Already Exists\")\n",
    "\n",
    "        '''Deals with TypeError: Object of type UUID is not JSON serializable by encoding the UUID'''\n",
    "        JSONEncoder_olddefault = JSONEncoder.default\n",
    "        def JSONEncoder_newdefault(self, o):\n",
    "            if isinstance(o, UUID): return str(o)\n",
    "            return JSONEncoder_olddefault(self, o)\n",
    "        JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "        '''Stores data by writing the 'recipe_details' dictionary to a JSON file called 'data.json' in the folder just created\n",
    "        The dicrionary is converted to a string using str() to deal with 'TypeError: Object of type WebElement is not JSON serializable'''\n",
    "        with open(os.path.join('raw_data', 'data.json'), 'w') as json_file:\n",
    "            json.dump(str(self.recipe_details), json_file)\n",
    "\n",
    "\n",
    "scraper.intitialize(scraper, 'https://www.pickuplimes.com', 'lemons', 5)\n",
    "#global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "#print(global_ids)\n",
    "\n",
    "\n",
    "\n",
    "# IDEALS\n",
    "# store all recipe images in a list\n",
    "# link getTitle() and home()\n",
    "# link getPageURL() and getUniqueID()\n",
    "# get search results\n",
    "# get search results on more than the first page\n",
    "# counts how many recipes there are\n",
    "# a for loop that automaticaly changes the xpath to get all the images for the recipe\n",
    "# bypass login\n",
    "# replace all XPaths with written XPatha\n",
    "# a method that makes a list containing all buttons and cycles through them looking for a link_text given as an argument\n",
    "# seperate website specific methods from general functioning\n",
    "# make sure all code works with any website so is reusable and genralisable"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4aa49906c49aab2699de75d29353ef6218c1548a647ded36ad1d3c83fbb1a8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataCollectionPipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
