{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Sugar Matcha Bubble Tea 0.jpg\n",
      "Brown Sugar Matcha Bubble Tea 1.jpg\n",
      "Brown Sugar Matcha Bubble Tea 2.jpg\n",
      "Brown Sugar Matcha Bubble Tea 3.jpg\n",
      "Brown Sugar Matcha Bubble Tea 4.jpg\n",
      "Brown Sugar Matcha Bubble Tea.json\n",
      "Strawberry Milk Bubble Tea 0.jpg\n",
      "Strawberry Milk Bubble Tea 1.jpg\n",
      "Strawberry Milk Bubble Tea 2.jpg\n",
      "Strawberry Milk Bubble Tea 3.jpg\n",
      "Strawberry Milk Bubble Tea 4.jpg\n",
      "Strawberry Milk Bubble Tea 5.jpg\n",
      "Strawberry Milk Bubble Tea 6.jpg\n",
      "Strawberry Milk Bubble Tea.json\n",
      "hi.json\n",
      "makeRaw_DataFolder Error\n",
      "raw_data Folder Already Exists\n",
      "makeRaw_DataFolder Error\n",
      "raw_data Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n",
      "makeImagesFolder Error\n",
      "Images Folder Already Exists\n",
      "makeRecipeFolder Error\n",
      "Recipe Folder Already Exists\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 909>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=905'>906</a>\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=906'>907</a>\u001b[0m             f\u001b[39m.\u001b[39mwrite(response\u001b[39m.\u001b[39mcontent)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=908'>909</a>\u001b[0m scraper\u001b[39m.\u001b[39;49mintitialize(scraper, \u001b[39m'\u001b[39;49m\u001b[39mhttps://www.pickuplimes.com\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlemons\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.intitialize\u001b[1;34m(self, url, searchTerm, delay)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=141'>142</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetURL(\u001b[39mself\u001b[39m, url) \u001b[39m# Have to start somewhere\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=142'>143</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__bucketDownloadJson(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttps://data-collection-pipeline-bucket.s3.eu-west-2.amazonaws.com/Brown+Sugar+Matcha+Bubble+Tea.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msomething.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=143'>144</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=144'>145</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcloseSession()\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=161'>162</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetAllRecipePages(\u001b[39mself\u001b[39m, data\u001b[39m.\u001b[39mcurrentURL)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=162'>163</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetRecipes(\u001b[39mself\u001b[39m, data\u001b[39m.\u001b[39mcurrentURL)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=163'>164</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcycleRecipeLinks(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=164'>165</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcloseSession()\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.cycleRecipeLinks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=176'>177</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mrecipeLinks:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=177'>178</a>\u001b[0m     data\u001b[39m.\u001b[39mcurrentURL \u001b[39m=\u001b[39m i\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=178'>179</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmakeImage(\u001b[39mself\u001b[39;49m, data\u001b[39m.\u001b[39;49mcurrentURL)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.makeImage\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=840'>841</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmakeImage\u001b[39m(\u001b[39mself\u001b[39m, url) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=841'>842</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=842'>843</a>\u001b[0m \u001b[39m    This function makes a file name for the image and downloads it.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=843'>844</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=852'>853</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=853'>854</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=855'>856</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetRecipeDetails(\u001b[39mself\u001b[39;49m, url)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=857'>858</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mrecipeDetails[\u001b[39m'\u001b[39m\u001b[39mImages\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=859'>860</a>\u001b[0m         \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m i:\n",
      "File \u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\decorators.py:11\u001b[0m, in \u001b[0;36mexceptionHandling.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func) \u001b[39m# maintains introspection\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     10\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     12\u001b[0m     \u001b[39mexcept\u001b[39;00m NoSuchElementException:\n\u001b[0;32m     13\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Exception: Element Not Found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.getRecipeDetails\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=643'>644</a>\u001b[0m \u001b[39m@decorators\u001b[39m\u001b[39m.\u001b[39mexceptionHandling\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=644'>645</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetRecipeDetails\u001b[39m(\u001b[39mself\u001b[39m, url) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=645'>646</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=646'>647</a>\u001b[0m \u001b[39m    This function calls the scrape functions\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=647'>648</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=656'>657</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=657'>658</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=658'>659</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetURL(\u001b[39mself\u001b[39;49m, url)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=660'>661</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscrapeName()  \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=661'>662</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscrapeTags()\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36mscraper.getURL\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=180'>181</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetURL\u001b[39m(\u001b[39mself\u001b[39m, url) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=181'>182</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=182'>183</a>\u001b[0m \u001b[39m    Navigates to a website using a url passed as a perameter.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=183'>184</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=188'>189</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=189'>190</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=191'>192</a>\u001b[0m     driver\u001b[39m.\u001b[39;49mget(url)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:437\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m    Loads a web page in the current browser session.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m'\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m'\u001b[39;49m: url})\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:423\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    420\u001b[0m         params[\u001b[39m'\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m    422\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_value(params)\n\u001b[1;32m--> 423\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[0;32m    424\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m    425\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:333\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    331\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[0;32m    332\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 333\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:355\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    352\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 355\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    356\u001b[0m     statuscode \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mstatus\n\u001b[0;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[0;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_body(\n\u001b[0;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[0;32m     80\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[0;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, u\u001b[39m.\u001b[39mrequest_uri, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[0;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\http\\client.py:1371\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1372\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\http\\client.py:319\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    321\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\http\\client.py:280\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 280\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    282\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd # used to create a database\n",
    "import functools # used to maintain introspection on decorators\n",
    "import requests\n",
    "import urllib\n",
    "import boto3 # used to access AWS resources\n",
    "import time\n",
    "import uuid # used to create a unique 'computer' id for each recipe\n",
    "import json # used to store the scraped details\n",
    "import os\n",
    "\n",
    "from uuid import UUID # used to create a unique id for each recipe\n",
    "from json import JSONEncoder # used to convert the UUID into a writable format\n",
    "from urllib.request import Request, urlopen # used to download images\n",
    "from sqlalchemy import create_engine # used to connect to the cloud\n",
    "\n",
    "#import secret_stash\n",
    "\n",
    "from sklearn.datasets import load_iris # used for learning and should be removed\n",
    "#from decorators import noSuchElementException\n",
    "import decorators\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#print(type(secret_stash))\n",
    "#print(type(secret_stash.secrets))\n",
    "#secret_key = secret_stash.get('SECRET_KEY')\n",
    "#secret_key = secrets.get('SECRET_KEY')\n",
    "#secret_key = secret_stash.get(secrets.get('SECRET_KEY'))\n",
    "\n",
    "DATABASE_TYPE = 'postgresql'\n",
    "DBAPI = 'psycopg2'\n",
    "ENDPOINT = 'datacollectionpipeline.c9nuz10uy6hn.us-east-1.rds.amazonaws.com' # Change it for your AWS endpoint\n",
    "USER = 'postgres'\n",
    "PASSWORD = 'Kilapila7'\n",
    "PORT = 5432\n",
    "DATABASE = 'postgres'\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "engine.connect()\n",
    "\n",
    "data = load_iris()\n",
    "iris = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "iris.head()\n",
    "\n",
    "iris.to_sql('iris_dataset', engine, if_exists='replace')\n",
    "df = pd.read_sql_table('iris_dataset', engine)\n",
    "df.head()\n",
    "\n",
    "# Displays bucket contents\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('data-collection-pipeline-bucket')\n",
    "for file in my_bucket.objects.all():\n",
    "    print(file.key)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "class data:\n",
    "    '''\n",
    "    This class contains all the attributes of the scraper\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    '''\n",
    "\n",
    "    articles = [] # Used to make a list of recipes\n",
    "    button = None # Used to interact with various button elements\n",
    "    container = None # Used to store various container elements\n",
    "    currentURL = \"\" # Used to store various urls \n",
    "    pages = [] # Used to append a list with pages links\n",
    "    recipeLinks = [] # Used to store recipe links\n",
    "    searchbar = None # Used to interact with search bar\n",
    "    source = \"\" # Used to get page source code\n",
    "    tag = None # Used to store various tag elements\n",
    "    title = \"\" # Used to get the title\n",
    "    totalPages = [] # Stores a list of pages\n",
    "\n",
    "    # File Management\n",
    "    count = 0 # Used in the creation of image filenames\n",
    "    parentDirectory = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/\" # Project directory\n",
    "    dataDirectory = \"\" # Used to create folder\n",
    "    imageDirectory = \"\" # Used to create folder \n",
    "    recipeDirectory = \"\" # Used to create modified folder names\n",
    "    imageFileName = \"\" # Used to create image files\n",
    "    jsonFileName = \"\" # Used to create json files\n",
    "\n",
    "    # Scraped Information\n",
    "    recipeDetails = {} # Used to store all the scraped recipe details\n",
    "    imageScrapeLimiter = 0 # Used to limit the amount of times an image is scraped\n",
    "\n",
    "    allergens = \"\" # Used to store scraped allergens\n",
    "    alternatives = \"\" #Used to store scraped alternatives\n",
    "    description = \"\" # Used to store the scraped description of the recipe\n",
    "    freeFrom = \"\" # Used to store the scraped free from information\n",
    "    imageLinks = [] # Used to scrape all of a recipes image links\n",
    "    ingredients = \"\" # Used to store the scraped ingredients\n",
    "    instructions = \"\" # Used to store scraped instructions\n",
    "    mainPhoto = None # Used to store main photo link\n",
    "    name = \"\" # Used to store scraped recipe name\n",
    "    notes = \"\" # Used to store scraped recipe notes\n",
    "    recipeTags = \"\" # Used to store scraped recipe tags\n",
    "    storage = \"\" # Used to store scraped storage instructions\n",
    "    timeCook = \"\" # Used to store scraped cook time\n",
    "    timePrep = \"\" # Used to store scraped recipe  prep time \n",
    "    timeTotal = \"\" # Used to store scraped total time it takes to make the recipe\n",
    "\n",
    "    # Cloud Management\n",
    "    bucket = 'data-collection-pipeline-bucket'\n",
    "\n",
    "class scraper:\n",
    "    '''\n",
    "    This class is the main scraper. \n",
    "\n",
    "    This class will navigate through a webpage and scrape data into a json file and images into a folder separated into recipes.\n",
    "\n",
    "    Attributes:\n",
    "    '''\n",
    "\n",
    "    def intitialize(self, url, searchTerm, delay):\n",
    "        '''\n",
    "        This function intitializes the scraper class.\n",
    "\n",
    "        Arg:\n",
    "            url (str): The target website\n",
    "            searchTerm (str): The word we want to type into the search bar\n",
    "            delay (int): The number of seconds used to delay a wait for elemnt and timeout exeptions\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "    \n",
    "        self.getURL(self, url) # Have to start somewhere\n",
    "        self.run(self)\n",
    "        self.closeSession() # Have to end somewhere\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        '''\n",
    "        This function calls all the necissary functions in order.\n",
    "\n",
    "        The order of function calls is for the class to navigate \n",
    "        through the website and scrape the text and images.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        #self.acceptCookies()\n",
    "        data.currentURL = self.findRecipeList(self)\n",
    "        self.getAllRecipePages(self, data.currentURL)\n",
    "        self.getRecipes(self, data.currentURL)\n",
    "        self.cycleRecipeLinks(self)\n",
    "        self.closeSession()   \n",
    "\n",
    "    def cycleRecipeLinks(self) -> None:\n",
    "        '''\n",
    "        This function iterates though the list of recipe urls, passing each url to the makeImage() function.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        for i in data.recipeLinks:\n",
    "            data.currentURL = i\n",
    "            self.makeImage(self, data.currentURL)\n",
    "\n",
    "    def getURL(self, url) -> None:\n",
    "        '''\n",
    "        Navigates to a website using a url passed as a perameter.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The target website\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        driver.get(url) \n",
    "\n",
    "    def getTitle() -> None:\n",
    "        '''\n",
    "        Fetches the title.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        data.title = driver.title\n",
    "\n",
    "    def closeSession() -> None:\n",
    "        '''\n",
    "        Closes the driver\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        driver.quit()\n",
    "\n",
    "    def getSourceCode() -> None:\n",
    "        '''\n",
    "        Fetches the current pages source code.\n",
    "        \n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.source = driver.page_source\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def search(self, searchTerm) -> None:\n",
    "        '''\n",
    "        Finds search bar and clicks it ready for input.\n",
    "        \n",
    "        Args:\n",
    "            searchTerm (str): The word passed to the findSearchBar() fucntion that types into the search box\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Find searchbar and click\n",
    "        button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'nav-searchbar-btn')))\n",
    "        button.click()\n",
    "\n",
    "        self.findSearchbar(self, searchTerm)\n",
    "\n",
    "    def searchbarTextAndClick(searchTerm) -> None:\n",
    "        '''\n",
    "        This function types the searchTerm into the searchbar and presses enter which then navigates to the search results page.\n",
    "\n",
    "        Args:\n",
    "            searchTerm (str): The word to type into the search box\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            data.searchbar.send_keys(searchTerm)\n",
    "            data.searchbar.send_keys(Keys.RETURN) # Return = Enter\n",
    "        except:\n",
    "            print(\"Exception: No search term input\")\n",
    "    \n",
    "    @decorators.exceptionHandling\n",
    "    def findSearchbar(self, searchTerm) -> None:\n",
    "        '''\n",
    "        This function finds the searchbar and calls the searchbarTextAndClick() function with the searchTerm parameter.\n",
    "\n",
    "        Args: \n",
    "            searchTerm (str): The word passed to the searchbarTextAndClick() fucntion that types into the search box\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        data.searchbar = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.NAME, \"sb\")))\n",
    "        self.searchbarTextAndClick(searchTerm)\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def home() -> None:\n",
    "        '''\n",
    "        Finds the title and clicks it.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        title = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'nav-image')))\n",
    "        title.click()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def findRecipeList(self) -> None:\n",
    "        '''\n",
    "        Finds the recipe tab and clicks it.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        data.button = WebDriverWait(driver,5).until(EC.presence_of_element_located((By.LINK_TEXT, 'Recipes')))\n",
    "        data.button.click()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def acceptCookies() -> None:\n",
    "        '''\n",
    "        Finds the accept cookies button and clicks it.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        data.button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/div[2]/div[2]')))\n",
    "        data.button.click()\n",
    "    \n",
    "    def getRecipes(self, url) -> None:\n",
    "        '''\n",
    "        Calls the functions to find the recipe container and puts all the recipes in a list.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The target website that contains all the recipe search results\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.getRecipeContainer()\n",
    "        self.makeRecipeList()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def getRecipeContainer() -> None:\n",
    "        '''\n",
    "        This function finds the recipe container\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.container = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='index-item-container']/div/div[2]/ul\"))) \n",
    "\n",
    "    def makeRecipeList() -> None:\n",
    "        '''\n",
    "        This function finds the individual recipe page link and stores it in a list\n",
    "\n",
    "        This function finds an individual recipes link by identifying the container \n",
    "        with all the recipes, then loops through that contain to find the individual \n",
    "        recipes and takes the url for that recipe and stores it in a list.\n",
    "        \n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.articles = data.container.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "        for i in data.articles:\n",
    "            data.tag = i.find_element(By.TAG_NAME, 'a')\n",
    "            data.recipeLinks.append(data.tag.get_attribute('href'))\n",
    "\n",
    "    def getPageURL() -> None:\n",
    "        '''\n",
    "        Returns the current page url.\n",
    "        \n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.currentURL =  driver.current_url\n",
    "\n",
    "    def getAllRecipePages(self, url) -> None:\n",
    "        '''\n",
    "        Calls the functions to navigate to each recipe page by modifying the current url and stores them in a list.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The target website that contains all the recipe search results\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.getTotalPages()\n",
    "        self.getSearchList()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def getTotalPages() -> None:\n",
    "        '''\n",
    "        This function counts the total number of page results.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        #totalPages = driver.find_element(By.CLASS_NAME, 'page-text') #actual\n",
    "        data.totalPages = [1, 2, 3] #temp to shorten runtime\n",
    "\n",
    "    def getSearchList() -> None:\n",
    "        '''\n",
    "        This function retrieves the url of each search result by modifying the url.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        for i in data.totalPages:\n",
    "            data.currentURL = driver.current_url\n",
    "            url_change = \"?page=\" + str(i)\n",
    "            next_page = data.currentURL + url_change\n",
    "            data.pages.append(next_page)\n",
    "\n",
    "    def getUniqueID(self, url) -> None:\n",
    "        '''\n",
    "        This function creates a uuid for each recipe by modifying  a url as a perameter. \n",
    "\n",
    "        Args: \n",
    "            url (str): The recipe page url\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        page_ID = url\n",
    "        just_ID = page_ID.replace(str(\"https://www.pickuplimes.com/recipe/\"), \"\")\n",
    "\n",
    "        ids = (just_ID, uuid.uuid4())\n",
    "\n",
    "        return ids\n",
    "\n",
    "    @decorators.scrapeHandling(data.name)\n",
    "    def scrapeName() -> None:\n",
    "        '''\n",
    "        This function scrapes the name of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.name = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/h1'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.recipeTags)\n",
    "    def scrapeTags() -> None:\n",
    "        '''\n",
    "        This function scrapes the tags of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.recipeTags = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"header-info-col\"]/div/header/a[1]/div/p'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.description)\n",
    "    def scrapeDescription() -> None:\n",
    "        '''\n",
    "        This function scrapes the description of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.description = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/span'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.timeTotal)\n",
    "    def scrapeTotalTime() -> None:\n",
    "        '''\n",
    "        This function scrapes the total cook time of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.timeTotal = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[2]'))).text  \n",
    "\n",
    "    @decorators.scrapeHandling(data.timePrep)\n",
    "    def scrapePrepTime() -> None:\n",
    "        '''\n",
    "        This function scrapes the prep time of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.timePrep = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[3]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.timeCook)\n",
    "    def scrapeCookTime() -> None:\n",
    "        '''\n",
    "        This function scrapes the cook time of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.timeCook = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[4]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.allergens)\n",
    "    def scrapeAllergens() -> None:\n",
    "        '''\n",
    "        This function scrapes the allergens of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.allergens = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[1]/div'))).text \n",
    "\n",
    "    @decorators.scrapeHandling(data.alternatives)\n",
    "    def scrapeAlternatives() -> None:\n",
    "        '''\n",
    "        This function scrapes the alternative ingrediants of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.alternatives = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[2]/div'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.freeFrom)\n",
    "    def scrapeFreeFrom() -> None:\n",
    "        '''\n",
    "        This function scrapes what the recipe is free from. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.freeFrom = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[3]/div'))).text \n",
    "\n",
    "    @decorators.scrapeHandling(data.ingredients)\n",
    "    def scrapeIngredients() -> None:\n",
    "        '''\n",
    "        This function scrapes the recipe ingredients. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.ingredients = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[2]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.instructions)\n",
    "    def scrapeInstructions() -> None:\n",
    "        '''\n",
    "        This function scrapes the instructions for the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.instructions = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ol'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.notes)\n",
    "    def scrapeNotes() -> None:\n",
    "        '''\n",
    "        This function scrapes the notes from the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.notes = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[1]/li'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.storage)\n",
    "    def scrapeStorage() -> None:\n",
    "        '''\n",
    "        This function scrapes the storage instructions of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.storage = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[2]/li'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.mainPhoto)\n",
    "    def scrapeMainPhoto() -> None:\n",
    "        '''\n",
    "        This function scrapes the main image from the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.mainPhoto = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main-image-container\"]/img')))\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def scrapeImages() -> None:\n",
    "        '''\n",
    "        This function scrapes the other images from the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        This function finds the image container then puts its children into a list. A limit is set based on the number of images found for later use. Finally the list is \n",
    "        iterated to get each images url link to create a list of image links\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        imageContainer = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-video\"]/div[2]'))) # Find the container\n",
    "        imageList = imageContainer.find_elements(By.XPATH, 'img') # Find the children\n",
    "        data.imageScrapeLimiter = len(imageList)\n",
    "\n",
    "        for i in imageList:\n",
    "            link = i.get_attribute('src')\n",
    "            data.imageLinks.append(link)\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def getRecipeDetails(self, url) -> None:\n",
    "        '''\n",
    "        This function calls the scrape functions\n",
    "\n",
    "        This function navigates to a recipe page and calls all the scrape functions to collect \n",
    "        the data from the page. It the calls the function that stores all the data in a dictionary \n",
    "        and calls the function that writes that dictionary to a json file\n",
    "\n",
    "        Args:\n",
    "            url (str): The recipe page url\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        self.getURL(self, url)\n",
    "\n",
    "        self.scrapeName()  \n",
    "        self.scrapeTags()\n",
    "        self.scrapeDescription()\n",
    "        self.scrapeTotalTime()\n",
    "        self.scrapePrepTime()\n",
    "        self.scrapeCookTime()\n",
    "        self.scrapeAllergens()\n",
    "        self.scrapeAlternatives()\n",
    "        self.scrapeFreeFrom()\n",
    "        self.scrapeIngredients()\n",
    "        self.scrapeInstructions()\n",
    "        self.scrapeNotes()\n",
    "        self.scrapeStorage()\n",
    "        self.scrapeMainPhoto()\n",
    "        self.scrapeImages()\n",
    "\n",
    "        self.storeDetails(self, url)\n",
    "        self.jsonFile(self)\n",
    "\n",
    "    def storeDetails(self, url) -> None:\n",
    "        '''\n",
    "        This function updates the data dictionarl with all the scraped information\n",
    "\n",
    "        Args: \n",
    "            url (str): The recipe url to make a unique ID\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        data.recipeDetails = {'ID': [], 'Name': [], 'Photo': [],'Tags': [], 'Description': [], 'Total Time': [], 'Prep Time': [], 'Cook Time': [], 'Allergens': [], 'Swaps': [], 'Free From': [], 'Ingredients': [], 'Directions': [], 'Notes': [], 'Storage': [], 'Images': []}\n",
    "        data.recipeDetails['ID'].append(self.getUniqueID(self, url))\n",
    "        data.recipeDetails['Name'].append(data.name)\n",
    "        data.recipeDetails['Photo'].append(data.mainPhoto)\n",
    "        data.recipeDetails['Tags'].append(data.recipeTags)\n",
    "        data.recipeDetails['Description'].append(data.description)\n",
    "        data.recipeDetails['Total Time'].append(data.timeTotal)\n",
    "        data.recipeDetails['Prep Time'].append(data.timePrep)\n",
    "        data.recipeDetails['Cook Time'].append(data.timeCook)\n",
    "        data.recipeDetails['Allergens'].append(data.allergens)\n",
    "        data.recipeDetails['Swaps'].append(data.alternatives)\n",
    "        data.recipeDetails['Free From'].append(data.freeFrom)\n",
    "        data.recipeDetails['Ingredients'].append(data.ingredients)\n",
    "        data.recipeDetails['Directions'].append(data.instructions)\n",
    "        data.recipeDetails['Notes'].append(data.notes)\n",
    "        data.recipeDetails['Storage'].append(data.storage)\n",
    "        data.recipeDetails['Images'].append(data.imageLinks)\n",
    "\n",
    "    def jsonFile(self) -> None:\n",
    "        '''This function creates a folder for the json file to be stored in\n",
    "        \n",
    "        This function creates a folder called 'raw_data' in the path for the \n",
    "        json file to be saved in. Uses a try except catch as it will throw an \n",
    "        error if the folder already exists.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.makeRaw_DataFolder()\n",
    "\n",
    "        # Deals with TypeError: Object of type UUID is not JSON serializable by encoding the UUID\n",
    "        JSONEncoder_olddefault = JSONEncoder.default\n",
    "        def JSONEncoder_newdefault(self, o):\n",
    "            if isinstance(o, UUID): return str(o)\n",
    "            return JSONEncoder_olddefault(self, o)\n",
    "        JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "        self.jsonDump(self)\n",
    "\n",
    "    @decorators.folderAlreadyExists(\"raw_data\")\n",
    "    def makeRaw_DataFolder():\n",
    "        '''\n",
    "        This function creates a folder.\n",
    "        \n",
    "        This function creates a folder for the json files to be stored in\n",
    "        Throws an exception if the folder already exists which is handeled \n",
    "        with a decorator.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.dataDirectory = \"raw_data\"\n",
    "        parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "        path = os.path.join(parent_dir, data.dataDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.dataDirectory)\n",
    "    \n",
    "    def jsonDump(self) -> None:\n",
    "        '''This function writes the dictionary data to a json file\n",
    "        \n",
    "        This function creates a name for the json file using the scraped recipe name and appending it with the file type\n",
    "        It stores data by writing the 'recipe_details' dictionary to a JSON file called 'data.json' in the folder just created\n",
    "        The dicrionary is converted to a string using str() to deal with 'TypeError: Object of type WebElement is not JSON serializable\n",
    "        The function to upload the json to the bucket is then called\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        data.jsonFileName = data.name + '.json'\n",
    "        path = os.path.join('raw_data', data.jsonFileName)\n",
    "        \n",
    "        with open(path, 'w') as json_file:\n",
    "            json.dump(str(data.recipeDetails), json_file, indent = 6)\n",
    "        \n",
    "        self.__bucketJson(self, path, data.jsonFileName)\n",
    "\n",
    "    def downloadImage(self, url) -> None:\n",
    "        '''\n",
    "        This function creates a folder.\n",
    "        \n",
    "        This function calls the function that creates a folder called 'images' \n",
    "        Then calls the function that creates a folder named after the recipes name \n",
    "        Adds User-Agent Headers in a try/catch exeption handler to bypass 403 error\n",
    "        Downloads the image into the folder of that recipe name\n",
    "        Calls the function that uploads the image to the bucket\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        self.makeImagesFolder()\n",
    "        self.makeRecipeFolder()\n",
    "\n",
    "        try:\n",
    "            # Adds headers to resolve 403 Fobidden Error\n",
    "            opener=urllib.request.build_opener()\n",
    "            opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "            urllib.request.install_opener(opener)\n",
    "            path = os.path.join(data.recipeDirectory, data.imageFileName + '.jpg')\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        except:\n",
    "            print(\"Error Downloading Images\")\n",
    "        \n",
    "        self.__bucketImage(self, path, data.imageFileName)\n",
    "\n",
    "    @decorators.folderAlreadyExists(\"Images\")\n",
    "    def makeImagesFolder() -> None:\n",
    "        '''\n",
    "        This function makes a folder.\n",
    "\n",
    "        This function Makes a folder called images for the recipe images to be stored in.\n",
    "        Uses a try except catch in a decorator as it will throw an error if the folders already exists.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.imageDirectory = \"images\"\n",
    "        path = os.path.join(data.parentDirectory, data.imageDirectory)\n",
    "        os.mkdir(path)\n",
    "\n",
    "    @decorators.folderAlreadyExists(\"Recipe\")\n",
    "    def makeRecipeFolder() -> None:\n",
    "        '''\n",
    "        This function makes a folder.\n",
    "\n",
    "        This function Makes a folder named after the recipe for the images to be stored in.\n",
    "        Uses a try except catch in a decorator as it will throw an error if the folders already exists.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.recipeDirectory = \"images/\" + data.name\n",
    "        path = os.path.join(data.parentDirectory, data.recipeDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.recipeDirectory)\n",
    "\n",
    "    def makeImage(self, url) -> None:\n",
    "        '''\n",
    "        This function makes a file name for the image and downloads it.\n",
    "        \n",
    "        This function retrieves the ID of each image using 'getRecipeDetails().\n",
    "        Removes all unecissary elements from the ID string to create a file name.\n",
    "        Pass the file name to 'downloadImages() to create a file.\n",
    "\n",
    "        Args:\n",
    "            url (str): The recipe url to scrape information from\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        self.getRecipeDetails(self, url)\n",
    "\n",
    "        for i in data.recipeDetails['Images']:\n",
    "\n",
    "            for j in i:\n",
    "                data.imageFileName = data.name + \" \" + str(data.count) + \".jpg\"\n",
    "                self.downloadImage(self, j)\n",
    "\n",
    "                if data.count < data.imageScrapeLimiter:\n",
    "                    data.count = data.count + 1\n",
    "                else:\n",
    "                    data.count = 0\n",
    "\n",
    "    def __bucketImage(self, path, imageFileName) -> None:\n",
    "        '''\n",
    "        This function uploads the image to the bucket\n",
    "        \n",
    "        Args:\n",
    "             path(str): The directory the image is stored in\n",
    "             imageFileName(str): The string used to name the image \n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        s3_client = boto3.client('s3')\n",
    "        response = s3_client.upload_file(path, data.bucket, imageFileName) # (file_name, bucket, object_name)\n",
    "    \n",
    "    def __bucketJson(self, path, jsonFileName) -> None:\n",
    "        '''\n",
    "        This function uploads the json to the bucket\n",
    "        \n",
    "        Args:\n",
    "             path(str): The directory the json is stored\n",
    "             jsonFileName(str): The string used to name the file \n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        s3_client = boto3.client('s3')\n",
    "        response = s3_client.upload_file(path, data.bucket, jsonFileName) # (file_name, bucket, object_name)\n",
    "\n",
    "    def __bucketDownloadJson(self, url, jsonFileName) -> None:\n",
    "        #Download via bucket url\n",
    "        response = requests.get(url, jsonFileName)\n",
    "        path = os.path.join('raw_data', jsonFileName)\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "scraper.intitialize(scraper, 'https://www.pickuplimes.com', 'lemons', 5)\n",
    "\n",
    "# IDEALS ---------------------------------------------------------------------------------------------------------------\n",
    "# link getTitle() and home()\n",
    "# link getPageURL() and getUniqueID()\n",
    "# get search results\n",
    "# get search results on more than the first page\n",
    "# make a separate main image\n",
    "# counts how many recipes there are on the site\n",
    "# a for loop that automaticaly changes the xpath to get all the images for the recipe\n",
    "# replace all XPaths with written XPaths\n",
    "# a method that makes a list containing all buttons and cycles through them looking for a link_text given as an argument\n",
    "# seperate website specific methods from general functioning in a class\n",
    "# make sure all code works with any website so is reusable and genralisable\n",
    "# use the unique id (page url) to stop scraping recipes it has already scraped \n",
    "# have a wait for element function that takes a xpath and a perameter"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89f331584aba9fd139438438d44e9f0981e5b7ec53d275c2a7e5fdd51f9e359f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataCollectionPipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
