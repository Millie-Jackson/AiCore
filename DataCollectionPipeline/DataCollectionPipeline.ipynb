{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 408>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=404'>405</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__downloadImage(\u001b[39mself\u001b[39m, j, data\u001b[39m.\u001b[39mrecipeName)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=405'>406</a>\u001b[0m                 data\u001b[39m.\u001b[39mcount \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcount \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m                \n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=407'>408</a>\u001b[0m scraper\u001b[39m.\u001b[39;49mintitialize(scraper, \u001b[39m'\u001b[39;49m\u001b[39mhttps://www.pickuplimes.com\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlemons\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1'\u001b[0m in \u001b[0;36mscraper.intitialize\u001b[1;34m(self, url, searchTerm, delay)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=75'>76</a>\u001b[0m global_ids \u001b[39m=\u001b[39m scraper\u001b[39m.\u001b[39m__getUniqueID(scraper, \u001b[39m'\u001b[39m\u001b[39mhttps://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=77'>78</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getURL(url) \u001b[39m# Have to start somewhere\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=78'>79</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__run(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=79'>80</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__closeSession()\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1'\u001b[0m in \u001b[0;36mscraper.__run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=81'>82</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__run\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=82'>83</a>\u001b[0m     \u001b[39m#self.__acceptCookies()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=83'>84</a>\u001b[0m     data\u001b[39m.\u001b[39mcurrentURL \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__findRecipeList(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=84'>85</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getAllRecipePages(\u001b[39mself\u001b[39m, data\u001b[39m.\u001b[39mcurrentURL)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=85'>86</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getRecipes(\u001b[39mself\u001b[39m, data\u001b[39m.\u001b[39mcurrentURL)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\decorators.py:11\u001b[0m, in \u001b[0;36mexceptionHandling.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func) \u001b[39m# maintains introspection\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     10\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     12\u001b[0m     \u001b[39mexcept\u001b[39;00m NoSuchElementException:\n\u001b[0;32m     13\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Exception: Element Not Found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1'\u001b[0m in \u001b[0;36mscraper.__findRecipeList\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=139'>140</a>\u001b[0m \u001b[39m'''Finds the recipe tab and clicks it.'''\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=140'>141</a>\u001b[0m data\u001b[39m.\u001b[39mbutton \u001b[39m=\u001b[39m WebDriverWait(driver,\u001b[39m5\u001b[39m)\u001b[39m.\u001b[39muntil(EC\u001b[39m.\u001b[39mpresence_of_element_located((By\u001b[39m.\u001b[39mLINK_TEXT, \u001b[39m'\u001b[39m\u001b[39mRecipes\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=141'>142</a>\u001b[0m data\u001b[39m.\u001b[39;49mbutton\u001b[39m.\u001b[39;49mclick()\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:81\u001b[0m, in \u001b[0;36mWebElement.click\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclick\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39m\"\"\"Clicks the element.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute(Command\u001b[39m.\u001b[39;49mCLICK_ELEMENT)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:710\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    708\u001b[0m     params \u001b[39m=\u001b[39m {}\n\u001b[0;32m    709\u001b[0m params[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id\n\u001b[1;32m--> 710\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent\u001b[39m.\u001b[39;49mexecute(command, params)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:423\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    420\u001b[0m         params[\u001b[39m'\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m    422\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_value(params)\n\u001b[1;32m--> 423\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[0;32m    424\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m    425\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:333\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    331\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[0;32m    332\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 333\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:355\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    352\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 355\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    356\u001b[0m     statuscode \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mstatus\n\u001b[0;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[0;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_body(\n\u001b[0;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[0;32m     80\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[0;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, u\u001b[39m.\u001b[39mrequest_uri, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[0;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\http\\client.py:1371\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1372\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\http\\client.py:319\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    321\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\http\\client.py:280\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 280\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    282\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import functools # used to maintain introspection on decorators\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "import uuid # used to create a unique 'computer' id for each recipe\n",
    "import json # used to store the scraped details\n",
    "import os\n",
    "\n",
    "from uuid import UUID # used to create a unique id for each recipe\n",
    "from json import JSONEncoder # used to convert the UUID into a writable format\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from decorators import exceptionHandling # used for genral exception handling\n",
    "from decorators import scrapeHandling # used for scraping specific exception handling\n",
    "from decorators import folderAlreadyExists # used for folder creation\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "class data:\n",
    "\n",
    "    articles = [] # Used to make a list of recipes\n",
    "    button = None # Used to interact with various button elements\n",
    "    container = None # Used to store various container elements\n",
    "    currentURL = \"\" # Used to store various urls \n",
    "    pages = [] # Used to append a list with pages links\n",
    "    recipeLinks = [] # Used to store recipe links\n",
    "    recipeName = \"\" # Stores the recipe name\n",
    "    searchbar = None # Used to interact with search bar\n",
    "    source = \"\" # Used to get page source code\n",
    "    tag = None # Used to store various tag elements\n",
    "    title = \"\" # Used to get the title\n",
    "    totalPages = [] # Stores a list of pages\n",
    "\n",
    "    # File Management\n",
    "    count = 0 # Used in the creation of image filenames\n",
    "    dataDirectory = \"\" # Used to create folder\n",
    "    imageDirectory = \"\" # Used to create folder \n",
    "    recipeDirectory = \"\" # Used to create modified folder names\n",
    "\n",
    "    # Scraped Information\n",
    "    recipeDetails = {} # Used to store all the scraped recipe details\n",
    "\n",
    "    allergens = \"\" # Used to store scraped allergens\n",
    "    alternatives = \"\" #Used to store scraped alternatives\n",
    "    description = \"\" # Used to store the scraped description of the recipe\n",
    "    freeFrom = \"\" # Used to store the scraped free from information\n",
    "    imageLinks = [] # Used to scrape all of a recipes image links\n",
    "    ingredients = \"\" # Used to store the scraped ingredients\n",
    "    instructions = \"\" # Used to store scraped instructions\n",
    "    mainPhoto = None # Used to store main photo link\n",
    "    name = \"\" # Used to store scraped recipe name\n",
    "    notes = \"\" # Used to store scraped recipe notes\n",
    "    recipeTags = \"\" # Used to store scraped recipe tags\n",
    "    storage = \"\" # Used to store scraped storage instructions\n",
    "    timeCook = \"\" # Used to store scraped cook time\n",
    "    timePrep = \"\" # Used to store scraped recipe  prep time \n",
    "    timeTotal = \"\" # Used to store scraped total time it takes to make the recipe\n",
    "\n",
    "class scraper:\n",
    "    def intitialize(self, url, searchTerm, delay):\n",
    "        global_ids = scraper.__getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "    \n",
    "        self.__getURL(url) # Have to start somewhere\n",
    "        self.__run(self)\n",
    "        self.__closeSession() # Have to end somewhere\n",
    "\n",
    "    def __run(self):\n",
    "        #self.__acceptCookies()\n",
    "        data.currentURL = self.__findRecipeList(self)\n",
    "        self.__getAllRecipePages(self, data.currentURL)\n",
    "        self.__getRecipes(self, data.currentURL)\n",
    "        self.__cycleRecipeLinks(self)\n",
    "        self.__closeSession()   \n",
    "\n",
    "    def __cycleRecipeLinks(self):\n",
    "        for i in data.recipeLinks:\n",
    "            data.currentURL = i\n",
    "            self.__makeImage(self, data.currentURL)\n",
    "\n",
    "    def __getURL(url):\n",
    "        '''Navigates to a website using a url passed as a perameter.'''\n",
    "        driver.get(url) \n",
    "\n",
    "    def __getTitle():\n",
    "        '''Fetches the title and prints it to screen.'''\n",
    "        data.title = driver.title\n",
    "\n",
    "    def __closeSession():\n",
    "        '''Closes the driver'''\n",
    "        driver.quit()\n",
    "\n",
    "    def __getSourceCode():\n",
    "        '''Fetches source code for the page.'''\n",
    "        data.source = driver.page_source\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __search(self, searchTerm):\n",
    "        '''Finds search bar, types in the search term which it takes as a perameter and clicks to navigate to the next page.'''\n",
    "        button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'nav-searchbar-btn')))\n",
    "        button.click()\n",
    "\n",
    "        self.findSearchbar(self, searchTerm)\n",
    "\n",
    "    def __searchbarTextAndClick(searchTerm):\n",
    "        try:\n",
    "            data.searchbar.send_keys(searchTerm)\n",
    "            data.searchbar.send_keys(Keys.RETURN) # Return = Enter\n",
    "        except:\n",
    "            print(\"Exception: No search term input\")\n",
    "    \n",
    "    @decorators.exceptionHandling\n",
    "    def __findSearchbar(self, searchTerm):\n",
    "\n",
    "        data.searchbar = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.NAME, \"sb\")))\n",
    "        self.__searchbarTextAndClick(searchTerm)\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __home():\n",
    "        '''Finds the title and clicks it.'''\n",
    "        title = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'nav-image')))\n",
    "        title.click()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __findRecipeList(self):\n",
    "        '''Finds the recipe tab and clicks it.'''\n",
    "        data.button = WebDriverWait(driver,5).until(EC.presence_of_element_located((By.LINK_TEXT, 'Recipes')))\n",
    "        data.button.click()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __acceptCookies():\n",
    "        '''Finds the accept cookies button and clicks it.'''\n",
    "        data.button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/div[2]/div[2]')))\n",
    "        data.button.click()\n",
    "    \n",
    "    def __getRecipes(self, url):\n",
    "        '''Finds the recipe container and puts all the recipes in a list.'''\n",
    "\n",
    "        self.__getRecipeContainer()\n",
    "        self.__makeRecipeList()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __getRecipeContainer():\n",
    "        data.container = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='index-item-container']/div/div[2]/ul\"))) \n",
    "\n",
    "    def __makeRecipeList():\n",
    "        data.articles = data.container.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "        for i in data.articles:\n",
    "            data.tag = i.find_element(By.TAG_NAME, 'a')\n",
    "            data.recipeLinks.append(data.tag.get_attribute('href'))\n",
    "\n",
    "    def __getPageURL():\n",
    "        '''Returns the current page url.'''\n",
    "        data.currentURL =  driver.current_url\n",
    "\n",
    "    def __getAllRecipePages(self, url):\n",
    "        '''Navigates to each recipe page by modifying the current url and stores them in a list.'''\n",
    "\n",
    "        self.__getTotalPages()\n",
    "        self.__getSearchList()\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __getTotalPages():\n",
    "         #totalPages = driver.find_element(By.CLASS_NAME, 'page-text') #actual\n",
    "        data.totalPages = [1, 2, 3] #temp to shorten runtime\n",
    "\n",
    "    def __getSearchList():\n",
    "        for i in data.totalPages:\n",
    "            data.currentURL = driver.current_url\n",
    "            url_change = \"?page=\" + str(i)\n",
    "            next_page = data.currentURL + url_change\n",
    "            data.pages.append(next_page)\n",
    "\n",
    "    def __getUniqueID(self, url):\n",
    "        '''Creates a uuid for each recipe taking a url as a perameter'''\n",
    "        \n",
    "        page_ID = url\n",
    "        just_ID = page_ID.replace(str(\"https://www.pickuplimes.com/recipe/\"), \"\")\n",
    "\n",
    "        ids = (just_ID, uuid.uuid4())\n",
    "\n",
    "    @decorators.scrapeHandling(data.name)\n",
    "    def __scrapeName():\n",
    "\n",
    "        data.name = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/h1'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.recipeTags)\n",
    "    def __scrapeTags():\n",
    "\n",
    "        data.recipeTags = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"header-info-col\"]/div/header/a[1]/div/p'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.description)\n",
    "    def __scrapeDescription():\n",
    "\n",
    "        data.description = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/span'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.timeTotal)\n",
    "    def __scrapeTotalTime():\n",
    "        \n",
    "        data.timeTotal = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[2]'))).text  \n",
    "\n",
    "    @decorators.scrapeHandling(data.timePrep)\n",
    "    def __scrapePrepTime():\n",
    "\n",
    "        data.timePrep = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[3]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.timeCook)\n",
    "    def __scrapeCookTime():\n",
    "        \n",
    "        data.timeCook = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[4]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.allergens)\n",
    "    def __scrapeAllergens():\n",
    "\n",
    "        data.allergens = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[1]/div'))).text \n",
    "\n",
    "    @decorators.scrapeHandling(data.alternatives)\n",
    "    def __scrapeAlternatives():\n",
    "        \n",
    "        data.alternatives = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[2]/div'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.freeFrom)\n",
    "    def __scrapeFreeFrom():\n",
    "        \n",
    "        data.freeFrom = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[3]/div'))).text \n",
    "\n",
    "    @decorators.scrapeHandling(data.ingredients)\n",
    "    def __scrapeIngredients():\n",
    "        \n",
    "        data.ingredients = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[2]'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.instructions)\n",
    "    def __scrapeInstructions():\n",
    "        \n",
    "        data.instructions = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ol'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.notes)\n",
    "    def __scrapeNotes():\n",
    "        \n",
    "        data.notes = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[1]/li'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.storage)\n",
    "    def __scrapeStorage():\n",
    "        \n",
    "        data.storage = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[2]/li'))).text\n",
    "\n",
    "    @decorators.scrapeHandling(data.mainPhoto)\n",
    "    def __scrapeMainPhoto():\n",
    "        \n",
    "        data.mainPhoto = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main-image-container\"]/img')))\n",
    "\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __scrapeImages():\n",
    "        imageContainer = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-video\"]/div[2]'))) # Find the container\n",
    "        imageList = imageContainer.find_elements(By.XPATH, 'img') # Find the children\n",
    "\n",
    "        for i in imageList:\n",
    "            link = i.get_attribute('src')\n",
    "            data.imageLinks.append(link)\n",
    "\n",
    "    @decorators.exceptionHandling\n",
    "    def __getRecipeDetails(self, url):\n",
    "        self.__getURL(url)\n",
    "\n",
    "        self.__scrapeName()  \n",
    "        self.__scrapeTags()\n",
    "        self.__scrapeDescription()\n",
    "        self.__scrapeTotalTime()\n",
    "        self.__scrapePrepTime()\n",
    "        self.__scrapeCookTime()\n",
    "        self.__scrapeAllergens()\n",
    "        self.__scrapeAlternatives()\n",
    "        self.__scrapeFreeFrom()\n",
    "        self.__scrapeIngredients()\n",
    "        self.__scrapeInstructions()\n",
    "        self.__scrapeNotes()\n",
    "        self.__scrapeStorage()\n",
    "        self.__scrapeMainPhoto()\n",
    "        self.__scrapeImages()\n",
    "\n",
    "        self.__storeDetails(self, url)\n",
    "        self.__jsonFile(self)\n",
    "\n",
    "    def __storeDetails(self, url):\n",
    "        data.recipeDetails = {'ID': [], 'Name': [], 'Photo': [],'Tags': [], 'Description': [], 'Total Time': [], 'Prep Time': [], 'Cook Time': [], 'Allergens': [], 'Swaps': [], 'Free From': [], 'Ingredients': [], 'Directions': [], 'Notes': [], 'Storage': [], 'Images': []}\n",
    "        data.recipeDetails['ID'].append(self.__getUniqueID(self, url))\n",
    "        data.recipeDetails['Name'].append(data.name)\n",
    "        data.recipeDetails['Photo'].append(data.mainPhoto)\n",
    "        data.recipeDetails['Tags'].append(data.recipeTags)\n",
    "        data.recipeDetails['Description'].append(data.description)\n",
    "        data.recipeDetails['Total Time'].append(data.timeTotal)\n",
    "        data.recipeDetails['Prep Time'].append(data.timePrep)\n",
    "        data.recipeDetails['Cook Time'].append(data.timeCook)\n",
    "        data.recipeDetails['Allergens'].append(data.allergens)\n",
    "        data.recipeDetails['Swaps'].append(data.alternatives)\n",
    "        data.recipeDetails['Free From'].append(data.freeFrom)\n",
    "        data.recipeDetails['Ingredients'].append(data.ingredients)\n",
    "        data.recipeDetails['Directions'].append(data.instructions)\n",
    "        data.recipeDetails['Notes'].append(data.notes)\n",
    "        data.recipeDetails['Storage'].append(data.storage)\n",
    "        data.recipeDetails['Images'].append(data.imageLinks)\n",
    "\n",
    "    def __jsonFile(self):\n",
    "        '''Creates a folder called 'raw_data' in the path for the json file to be saved in\n",
    "        Uses a try except catch as it will throw an error if the folder already exists'''\n",
    "\n",
    "        self.__makeRaw_DataFolder()\n",
    "\n",
    "        '''Deals with TypeError: Object of type UUID is not JSON serializable by encoding the UUID'''\n",
    "        JSONEncoder_olddefault = JSONEncoder.default\n",
    "        def __JSONEncoder_newdefault(self, o):\n",
    "            if isinstance(o, UUID): return str(o)\n",
    "            return JSONEncoder_olddefault(self, o)\n",
    "        JSONEncoder.default = __JSONEncoder_newdefault\n",
    "\n",
    "        self.__jsonDump()\n",
    "\n",
    "    @decorators.folderAlreadyExists(\"raw_data\")\n",
    "    def __makeRaw_DataFolder():\n",
    "        \n",
    "        data.dataDirectory = \"raw_data\"\n",
    "        parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "        path = os.path.join(parent_dir, data.dataDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.dataDirectory)\n",
    "    \n",
    "    def __jsonDump():\n",
    "        '''Stores data by writing the 'recipe_details' dictionary to a JSON file called 'data.json' in the folder just created\n",
    "        The dicrionary is converted to a string using str() to deal with 'TypeError: Object of type WebElement is not JSON serializable'''\n",
    "        with open(os.path.join('raw_data', 'data.json'), 'w') as json_file:\n",
    "            json.dump(str(data.recipeDetails), json_file)\n",
    "\n",
    "    def __downloadImage(self, url, recipeName):\n",
    "        '''Creates a folder called 'images' and another with the recipe name in the path for the image files to be saved in\n",
    "        Uses a try except catch as it will throw an error if the folders already exists\n",
    "        Adds User-Agent Headers to bypass 403 error\n",
    "        Downloads the images into the folder of that recipe name'''\n",
    "\n",
    "        self.__makeImagesFolder()\n",
    "        self.__makeRecipeFolder()\n",
    "        \n",
    "        try:\n",
    "            # Adds headers to resolve 403 Fobidden Error\n",
    "            opener=urllib.request.build_opener()\n",
    "            opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "            urllib.request.install_opener(opener)\n",
    "\n",
    "            downloadDirectory = \"images/\" + data.recipeDirectory + \"/\"\n",
    "            fileType = '.jpg'\n",
    "            fileName = downloadDirectory + recipeName + fileType\n",
    "            image = urllib.request.urlretrieve(url, fileName)\n",
    "        except:\n",
    "            print(\"Error Downloading Images\")           \n",
    "\n",
    "    @decorators.folderAlreadyExists(\"Images\")\n",
    "    def __makeImagesFolder():\n",
    "        \n",
    "        data.imageDirectory = \"images\"\n",
    "        parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "        path = os.path.join(parent_dir, data.imageDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.imageDirectory)\n",
    "\n",
    "    @decorators.folderAlreadyExists(\"Recipe\")\n",
    "    def __makeRecipeFolder():\n",
    "\n",
    "        data.recipeDirectory = data.recipeName.replace(\".jpg\", \"\").replace(\"0\", \"\").replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\").replace(\"8\", \"\").replace(\"9\", \"\")\n",
    "        parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/images\"\n",
    "        path = os.path.join(parent_dir, data.recipeDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.recipeDirectory)\n",
    "\n",
    "    def __makeImage(self, url):\n",
    "        '''Retrieves the ID of each image using 'getRecipeDetails()\n",
    "        Removes all unecissary elements from the ID string to create a file name\n",
    "        Pass the file name to 'downloadImages() to create a file'''\n",
    "        \n",
    "        self.__getRecipeDetails(self, url)\n",
    "        \n",
    "        for i in data.recipeDetails['Images']:\n",
    "            for j in i:\n",
    "                IDtoName = str(data.recipeDetails['ID']).split()\n",
    "                IDtoName1 = str(IDtoName[0]).replace(\"(\", \"\")\n",
    "                IDtoName2 = str(IDtoName1).replace(\"[\", \"\")\n",
    "                IDtoName3 = str(IDtoName2).replace(\",\", \"\")\n",
    "                IDtoName4 = str(IDtoName3).replace(\"'\", \"\")\n",
    "\n",
    "                data.recipeName = IDtoName4 + \"-\" + str(data.count) + \".jpg\"\n",
    "                self.__downloadImage(self, j, data.recipeName)\n",
    "                data.count = data.count + 1                \n",
    "\n",
    "scraper.intitialize(scraper, 'https://www.pickuplimes.com', 'lemons', 5)\n",
    "\n",
    "\n",
    "# IDEALS ---------------------------------------------------------------------------------------------------------------\n",
    "# link getTitle() and home()\n",
    "# link getPageURL() and getUniqueID()\n",
    "# get search results\n",
    "# get search results on more than the first page\n",
    "# make a separate main image\n",
    "# counts how many recipes there are on the site\n",
    "# a for loop that automaticaly changes the xpath to get all the images for the recipe\n",
    "# bypass login\n",
    "# replace all XPaths with written XPaths\n",
    "# a method that makes a list containing all buttons and cycles through them looking for a link_text given as an argument\n",
    "# seperate website specific methods from general functioning in a class\n",
    "# make sure all code works with any website so is reusable and genralisable\n",
    "# use the unique id (page url) to stop scraping recipes it has already scraped \n",
    "# have a wait for element function that takes a xpath and a perameter\n",
    "# time each function to optimise run speed\n",
    "# error handeling function? all error hadling is done by one function?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89f331584aba9fd139438438d44e9f0981e5b7ec53d275c2a7e5fdd51f9e359f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataCollectionPipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
