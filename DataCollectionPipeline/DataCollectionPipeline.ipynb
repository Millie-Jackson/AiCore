{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': [('spicy-garlic-wok-noodles-213', UUID('72d77574-cf13-4a5a-bd1e-5009b6b99a20'))], 'Name': ['Spicy Garlic Wok Noodles'], 'Tags': ['Meal'], 'Description': [\"This is a new and improved update on our favourite spicy garlic wok noodles! It's loaded with veggies, and is an ultra-flexible recipe. Change up the veggies based on what you have in the fridge - green beans, cabbage, carrots, mushrooms, or bean sprouts - they all work great!\"], 'Total Time': ['Total\\n40 min'], 'Prep Time': ['Prep\\n15 min'], 'Cook Time': ['Cook\\n25 min'], 'Allergens': ['soy\\ngluten'], 'Swaps': ['sesame'], 'Free From': ['peanut\\ntree nut'], 'Ingredients': ['Ingredients\\nSeasoned tofu\\n1 Tbsp (15 mL) vegetable oil\\n11.4 oz (325 g) extra firm tofu, pressed, cut into 1 cm cubes\\n1 Tbsp (15 mL) ketjap manis*\\nStir fry\\nÂ½ Tbsp (7 mL) sesame oil\\n3 medium shallots, minced\\n5 cloves garlic, minced\\n1 small broccoli, cut into bite-sized florets\\n6.5 oz (185 g) quick-cooking ramen noodles\\n1 red bell pepper, minced\\n1 cup (149 g) cherry tomatoes\\n2 Tbsp (30 mL) dark soy sauce\\n2 Tbsp (30 mL) sambal oelek, or less for less spice\\nOptional garnish\\nsliced green onion\\ntoasted sesame seeds\\nBuy ingredients on AmazonFresh  '], 'Directions': ['To a large pan on high heat, add the oil. When hot, add the tofu cubes. Stir every few minutes to ensure the tofu cooks evenly on all sides, but careful to not over-stir as this can cause the tofu to crumble. When lightly golden on all sides, add the ketjap, toss to coat, and remove from the heat.\\nTo a large pot or wok on medium-high heat, add the sesame oil, shallots, garlic, and broccoli and cook for about 6 minutes.\\nTo a large heat-proof bowl or pot, add the ramen noodles and pour boiling water over top. Cover and let sit to soften for 4 - 5 minutes. We ideally want to drain when the noodles are 2 minutes away from being al dente. If using noodles that are not quick-cooking, simply cook according to the package instructions, and drain 2 minutes before they are fully cooked. Run the noodles under cold water.\\nAdd the drained noodles to the large pot containing the broccoli and shallot mixture. Also add in the bell pepper, cherry tomatoes, soy sauce, and sambal. Toss everything to coat, and continue to cook until the noodles are fully cooked and everything is heated through.\\nServe in your favourite bowls, top with desired garnish, and enjoy!'], 'Notes': ['* Ketjap is a slightly thick, sweetened soy sauce commonly used in Indonesian cuisine.'], 'Storage': ['Store in an airtight container in the fridge for up to 3 days.'], 'Image': [[<selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"fec4534d-21d0-4fea-9a8c-8dd950bc7931\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"0fcf7de0-d87e-4edc-899e-10a52156c8ef\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"e21cdc3f-293b-47c8-8a89-b9fc81ab360d\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"72c82399-b4cf-41bc-aeb6-c5be7882ae41\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"ffb741e3-3516-4716-8b68-15f962ddb26a\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"2438d780-1dff-4226-a5f3-4cfbd06f89da\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"983e3853-1221-48a4-bb41-2d2122aedbfb\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"b154b9a5-35e2-45da-b4a7-74ea0a54af50\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"63ba2524e159d43ac83550feea873463\", element=\"c87a3f4e-ab8a-4a85-90d8-f2ec3e7b3bb9\")>]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "class scraper:\n",
    "    def intitialize(self, url, search_term):\n",
    "        global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "        self.getURL(url)\n",
    "        #self.getTitle()\n",
    "        #self.acceptCookies()\n",
    "        #self.getAllRecipePages()\n",
    "        #self.getSourceCode()\n",
    "        #self.search(search_term)\n",
    "        #time.sleep(3)\n",
    "        #self.home()\n",
    "        #self.findRecipeList()\n",
    "        #self.getRecipeDetails()\n",
    "        #self.getRecipes()\n",
    "        #self.getPageURL()\n",
    "        #self.getUniqueID()\n",
    "        self.getRecipeDetails(self)\n",
    "        #time.sleep(3)\n",
    "        self.closeSession()\n",
    "\n",
    "    def getURL(url):\n",
    "        driver.get(url) \n",
    "\n",
    "    def getTitle():\n",
    "        print(driver.title)\n",
    "\n",
    "    def closeSession():\n",
    "        time.sleep(3)\n",
    "        driver.quit()\n",
    "\n",
    "    def getSourceCode():\n",
    "        print(driver.page_source)\n",
    "    \n",
    "    def search(search_term):\n",
    "        try:\n",
    "            button = driver.find_element(By.ID, 'nav-searchbar-btn')\n",
    "            button.click()\n",
    "            try:\n",
    "                search_bar = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, \"sb\")))\n",
    "                try:\n",
    "                    search_bar.send_keys(search_term)\n",
    "                    search_bar.send_keys(Keys.RETURN) # Return = Enter\n",
    "                except:\n",
    "                    print(\"Exception: No search term input\")\n",
    "            except:\n",
    "                print(\"Exception: No search bar found\")\n",
    "        except:\n",
    "            print(\"Exception: No search button found\")\n",
    "\n",
    "    def home():\n",
    "        title = driver.find_element(By.ID, 'nav-image')\n",
    "        title.click()\n",
    "\n",
    "    def findRecipeList():\n",
    "        button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.LINK_TEXT, 'Recipes')))\n",
    "        button.click()\n",
    "\n",
    "    def acceptCookies():\n",
    "        try:\n",
    "            cookie_button = driver.find_element(By.XPATH, '/html/body/div/div[2]/div[2]')\n",
    "            cookie_button.click()\n",
    "            print(\"Removed Cookies\")\n",
    "        except:\n",
    "            print(\"Exeption: Didnt Find Cookie Button\")\n",
    "    \n",
    "    def getRecipes():\n",
    "        main = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'index-item-container')))      \n",
    "        print(\"Found results\")\n",
    "        articles = main.find_elements(By.TAG_NAME, 'li')\n",
    "        print(\"Number of Recipes:\", len(articles))\n",
    "\n",
    "        for i in articles:\n",
    "            print(\"Recipe:\" , i.text)\n",
    "\n",
    "    def getPageURL():\n",
    "        print(\"URL:\", driver.current_url)\n",
    "\n",
    "    def getAllRecipePages(self):\n",
    "        pages = []\n",
    "        self.getURL('https://www.pickuplimes.com/')\n",
    "        scraper.findRecipeList()\n",
    "        page = [driver.current_url]\n",
    "\n",
    "        #total_pages = driver.find_element(By.CLASS_NAME, 'page-text')\n",
    "\n",
    "        total_pages = [1, 2, 3, 4, 5]\n",
    "\n",
    "        for i in total_pages:\n",
    "            current_page = driver.current_url\n",
    "            url_change = \"?page=\" + str(i)\n",
    "            next_page = current_page + url_change\n",
    "            print(next_page)\n",
    "            pages.append(next_page)\n",
    "            print(\"Number of Pages:\", len(pages))\n",
    "\n",
    "    def getUniqueID(self, url):\n",
    "        \n",
    "        page_ID = url\n",
    "        just_ID = page_ID.replace(str(\"https://www.pickuplimes.com/recipe/\"), \"\")\n",
    "\n",
    "        ids = (just_ID, uuid.uuid4())\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def getRecipeDetails(self):\n",
    "        self.getURL('https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "\n",
    "        name = driver.find_element(By.XPATH, '//*[@id=\"header-info-col\"]/div/header/h1').text\n",
    "        tag = driver.find_element(By.XPATH, '//*[@id=\"header-info-col\"]/div/header/a[1]/div/p').text\n",
    "        description = driver.find_element(By.XPATH, '//*[@id=\"header-info-col\"]/div/header/span').text\n",
    "        time_total = driver.find_element(By.XPATH, '//*[@id=\"recipe-info-container\"]/div[2]').text\n",
    "        time_prep = driver.find_element(By.XPATH, '//*[@id=\"recipe-info-container\"]/div[3]').text\n",
    "        time_cook = driver.find_element(By.XPATH, '//*[@id=\"recipe-info-container\"]/div[4]').text\n",
    "        allergens = driver.find_element(By.XPATH, '//*[@id=\"allergen-info-container\"]/div[1]/div').text \n",
    "        swap = driver.find_element(By.XPATH, '//*[@id=\"allergen-info-container\"]/div[2]/div').text\n",
    "        free_from = driver.find_element(By.XPATH, '//*[@id=\"allergen-info-container\"]/div[3]/div').text  \n",
    "        ingredients = driver.find_element(By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[2]').text\n",
    "        directions = driver.find_element(By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ol').text\n",
    "        notes = driver.find_element(By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[1]/li').text\n",
    "        storage = driver.find_element(By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[2]/li').text\n",
    "\n",
    "        picture_main = driver.find_element(By.XPATH, '//*[@id=\"main-image-container\"]/img')\n",
    "        picture1 = driver.find_element(By.XPATH, '//*[@id=\"recipe-video\"]/div[2]/img[1]')\n",
    "        picture2 = driver.find_element(By.XPATH, '//*[@id=\"recipe-video\"]/div[2]/img[2]')\n",
    "        picture3 = driver.find_element(By.XPATH, '//*[@id=\"recipe-video\"]/div[2]/img[3]')\n",
    "        picture4 = driver.find_element(By.XPATH, '//*[@id=\"recipe-video\"]/div[2]/img[4]')\n",
    "        picture5 = driver.find_element(By.XPATH, '//*[@id=\"recipe-video\"]/div[2]/img[5]')\n",
    "        picture6 = driver.find_element(By.XPATH, '//*[@id=\"recipe-video\"]/div[2]/img[6]')\n",
    "        picture7 = driver.find_element(By.XPATH, '//*[@id=\"recipe-video\"]/div[2]/img[7]')\n",
    "        picture8 = driver.find_element(By.XPATH, '//*[@id=\"recipe-video\"]/div[2]/img[8]')\n",
    "        pictures = [picture_main, picture1, picture2, picture3, picture4, picture5, picture6, picture7, picture8]\n",
    "        \n",
    "        recipe_details = {'ID': [], 'Name': [], 'Tags': [], 'Description': [], 'Total Time': [], 'Prep Time': [], 'Cook Time': [], 'Allergens': [], 'Swaps': [], 'Free From': [], 'Ingredients': [], 'Directions': [], 'Notes': [], 'Storage': [], 'Image': []}\n",
    "        recipe_details['ID'].append(self.getUniqueID(self, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213'))\n",
    "        recipe_details['Name'].append(name)\n",
    "        recipe_details['Tags'].append(tag)\n",
    "        recipe_details['Description'].append(description)\n",
    "        recipe_details['Total Time'].append(time_total)\n",
    "        recipe_details['Prep Time'].append(time_prep)\n",
    "        recipe_details['Cook Time'].append(time_cook)\n",
    "        recipe_details['Allergens'].append(allergens)\n",
    "        recipe_details['Swaps'].append(swap)\n",
    "        recipe_details['Free From'].append(free_from)\n",
    "        recipe_details['Ingredients'].append(ingredients)\n",
    "        recipe_details['Directions'].append(directions)\n",
    "        recipe_details['Notes'].append(notes)\n",
    "        recipe_details['Storage'].append(storage)\n",
    "        recipe_details['Image'].append(pictures)\n",
    "        recipe_details\n",
    "        print(recipe_details)\n",
    "\n",
    "scraper.intitialize(scraper, 'https://www.pickuplimes.com', 'lemons')\n",
    "#global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "#print(global_ids)\n",
    "#scraper.closeSession()\n",
    "\n",
    "# IDEALS\n",
    "# store all recipe images in a list\n",
    "# link getTitle() and home()\n",
    "# link getPageURL() and getUniqueID()\n",
    "# get search results\n",
    "# get search results on more than the first page\n",
    "# counts how many recipes there are\n",
    "# a for loop that automaticaly changes the xpath to get all the images for the recipe\n",
    "# bypass login\n",
    "# store all recipe unique id in a list\n",
    "# replace all XPaths\n",
    "# a method that makes a list containing all buttons and cycles through them looking for a link_text given as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to frame\n",
      "Accepted Cookies\n",
      "https://www.zoopla.co.uk/for-sale/details/61207984/?search_identifier=c14ac08bf282507f6273830c65fcde12\n",
      "{'Price': ['Â£475,000'], 'Address': ['Langdale Road, Thornton Heath, Surrey CR7'], 'Bedrooms': ['3 beds'], 'Description': ['This mid-terrace house, which is in need of some modernisation, would make an ideal family home.\\n\\nThe ground floor comprises two reception rooms, kitchen, and a lean too with access to the private rear garden from both the kitchen and the rear reception.\\n\\nUpstairs has two double bedrooms and a good-sized single bedroom as well as a second bathroom with three-piece suite.\\n\\nThe property is situated within walking distance to local amenities and schools as well as Thornton Heath Station.']}\n"
     ]
    }
   ],
   "source": [
    "# ZOOPLA\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "url = \"https://zoopla.co.uk\" \n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5) # Slow it down otherwise it will miss the button \n",
    "\n",
    "try: \n",
    "    frame_id = \"gdpr-consent-notice\"\n",
    "    driver.switch_to.frame(frame_id)\n",
    "    print(\"Switched to frame\")\n",
    "    cookie_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
    "    cookie_button.click()\n",
    "    print(\"Accepted Cookies\")\n",
    "except:\n",
    "    print(\"No Cookie Button\")\n",
    "    pass\n",
    "\n",
    "url = \"https://www.zoopla.co.uk/for-sale/property/london/?q=london&results_sort=newest_listings&search_source=home\"\n",
    "driver.get(url)\n",
    "\n",
    "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61207984\"]/div[1]/div[2]')\n",
    "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
    "link = a_tag.get_attribute('href')\n",
    "driver.get(link)\n",
    "print(link)\n",
    "\n",
    "price = driver.find_element(By.XPATH, '//span[@data-testid=\"price\"]').text \n",
    "address = driver.find_element(By.XPATH, '//span[@data-testid=\"address-label\"]').text \n",
    "bedrooms = driver.find_element(By.XPATH, '//span[@data-testid=\"beds-label\"]').text  \n",
    "\n",
    "div_tag = driver.find_element(By.XPATH, '//div[@data-testid=\"truncated_text_container\"]') \n",
    "span_tag = div_tag.find_element(By.XPATH,'.//span') \n",
    "description = span_tag.text \n",
    "driver.find_element(By.CSS_SELECTOR)\n",
    "\n",
    "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []} \n",
    "dict_properties['Price'].append(price) \n",
    "dict_properties['Address'].append(address) \n",
    "dict_properties['Bedrooms'].append(bedrooms) \n",
    "dict_properties['Description'].append(description) \n",
    "dict_properties\n",
    "print(dict_properties)\n",
    "\n",
    "time.sleep(3)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1, 2, 3]\n",
    "print(my_list * 2)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4aa49906c49aab2699de75d29353ef6218c1548a647ded36ad1d3c83fbb1a8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataCollectionPipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
