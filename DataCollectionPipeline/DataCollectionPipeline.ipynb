{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=36'>37</a>\u001b[0m DATABASE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpostgres\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=38'>39</a>\u001b[0m engine \u001b[39m=\u001b[39m create_engine(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDATABASE_TYPE\u001b[39m}\u001b[39;00m\u001b[39m+\u001b[39m\u001b[39m{\u001b[39;00mDBAPI\u001b[39m}\u001b[39;00m\u001b[39m://\u001b[39m\u001b[39m{\u001b[39;00mUSER\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00mPASSWORD\u001b[39m}\u001b[39;00m\u001b[39m@\u001b[39m\u001b[39m{\u001b[39;00mENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00mPORT\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mDATABASE\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=39'>40</a>\u001b[0m engine\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=41'>42</a>\u001b[0m data \u001b[39m=\u001b[39m load_iris()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=42'>43</a>\u001b[0m iris \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m], columns\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mfeature_names\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3210\u001b[0m, in \u001b[0;36mEngine.connect\u001b[1;34m(self, close_with_result)\u001b[0m\n\u001b[0;32m   3195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, close_with_result\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   3196\u001b[0m     \u001b[39m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[0;32m   3197\u001b[0m \n\u001b[0;32m   3198\u001b[0m \u001b[39m    The :class:`_engine.Connection` object is a facade that uses a DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3207\u001b[0m \n\u001b[0;32m   3208\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3210\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection_cls(\u001b[39mself\u001b[39;49m, close_with_result\u001b[39m=\u001b[39;49mclose_with_result)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\engine\\base.py:96\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, close_with_result, _branch_from, _execution_options, _dispatch, _has_events, _allow_revalidate)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39m=\u001b[39m _branch_from\u001b[39m.\u001b[39m_has_events\n\u001b[0;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         connection\n\u001b[0;32m     95\u001b[0m         \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m         \u001b[39melse\u001b[39;00m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transaction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_transaction \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__savepoint_seq \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3289\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self, _connection)\u001b[0m\n\u001b[0;32m   3267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraw_connection\u001b[39m(\u001b[39mself\u001b[39m, _connection\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   3268\u001b[0m     \u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3269\u001b[0m \n\u001b[0;32m   3270\u001b[0m \u001b[39m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3287\u001b[0m \n\u001b[0;32m   3288\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_pool_connect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect, _connection)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3256\u001b[0m, in \u001b[0;36mEngine._wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3254\u001b[0m dialect \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialect\n\u001b[0;32m   3255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3256\u001b[0m     \u001b[39mreturn\u001b[39;00m fn()\n\u001b[0;32m   3257\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mdbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   3258\u001b[0m     \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\pool\\base.py:310\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    303\u001b[0m     \u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[39m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\pool\\base.py:868\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    866\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_checkout\u001b[39m(\u001b[39mcls\u001b[39m, pool, threadconns\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, fairy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    867\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[1;32m--> 868\u001b[0m         fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[0;32m    870\u001b[0m         fairy\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m pool\n\u001b[0;32m    871\u001b[0m         fairy\u001b[39m.\u001b[39m_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\pool\\base.py:476\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    475\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheckout\u001b[39m(\u001b[39mcls\u001b[39m, pool):\n\u001b[1;32m--> 476\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[0;32m    477\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         dbapi_connection \u001b[39m=\u001b[39m rec\u001b[39m.\u001b[39mget_connection()\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\pool\\impl.py:146\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m         \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m--> 146\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_get()\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[39m.\u001b[39;49mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[39m=\u001b[39;49mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m compat\u001b[39m.\u001b[39mpy3k \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info[\u001b[39m1\u001b[39m]:\n\u001b[0;32m     76\u001b[0m         \u001b[39m# emulate Py3K's behavior of telling us when an exception\u001b[39;00m\n\u001b[0;32m     77\u001b[0m         \u001b[39m# occurs in an exception handler.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\util\\compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    204\u001b[0m     exception\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m replace_context\n\u001b[0;32m    206\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m    208\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\pool\\impl.py:143\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inc_overflow():\n\u001b[0;32m    142\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[0;32m    144\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m         \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\pool\\base.py:256\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    254\u001b[0m     \u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\pool\\base.py:371\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pool \u001b[39m=\u001b[39m pool\n\u001b[0;32m    370\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[1;32m--> 371\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[0;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\pool\\base.py:661\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 661\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    662\u001b[0m     pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n\u001b[0;32m    663\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfresh \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\engine\\create.py:590\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[1;32m--> 590\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39mconnect(\u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\sqlalchemy\\engine\\default.py:597\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams):\n\u001b[0;32m    596\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi\u001b[39m.\u001b[39mconnect(\u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams)\n",
      "File \u001b[1;32mc:\\Users\\Millie\\miniconda3\\envs\\DataCollectionPipeline\\lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39mconnection_factory, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd # used to create a database\n",
    "import functools # used to maintain introspection on decorators\n",
    "import requests\n",
    "import urllib\n",
    "import boto3 # used to access AWS resources\n",
    "import time\n",
    "import uuid # used to create a unique 'computer' id for each recipe\n",
    "import json # used to store the scraped details\n",
    "import os\n",
    "\n",
    "from uuid import UUID # used to create a unique id for each recipe\n",
    "from json import JSONEncoder # used to convert the UUID into a writable format\n",
    "from urllib.request import Request, urlopen # used to download images\n",
    "from sqlalchemy import create_engine # used to connect to the cloud\n",
    "\n",
    "from sklearn.datasets import load_iris # used for learning and should be removed\n",
    "#from decorators import noSuchElementException\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "DATABASE_TYPE = 'postgresql'\n",
    "DBAPI = 'psycopg2'\n",
    "ENDPOINT = 'datacollectionpipeline.c9nuz10uy6hn.us-east-1.rds.amazonaws.com' # Change it for your AWS endpoint\n",
    "USER = 'postgres'\n",
    "PASSWORD = 'Kilapila7'\n",
    "PORT = 5432\n",
    "DATABASE = 'postgres'\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "engine.connect()\n",
    "\n",
    "data = load_iris()\n",
    "iris = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "iris.head()\n",
    "\n",
    "iris.to_sql('iris_dataset', engine, if_exists='replace')\n",
    "df = pd.read_sql_table('iris_dataset', engine)\n",
    "df.head()\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "# (file_name, bucket, object_name)\n",
    "response = s3_client.upload_file('raw_data/data.json', 'data-collection-pipeline-bucket', 'data.json') \n",
    "response = s3_client.upload_file('images/Strawberry Milk Bubble Tea/Strawberry Milk Bubble Tea 0.jpg.jpg', 'data-collection-pipeline-bucket', 'images')\n",
    "#files = 'images/'\n",
    "#for file in files:\n",
    "    #imageJPG = os.path.join('images/', file)\n",
    "    #s3_client.upload_file(imageJPG, 'data-collection-pipeline-bucket', 'images') #(directory, selected_text)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "class data:\n",
    "    '''\n",
    "    This class contains all the attributes of the scraper\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    '''\n",
    "\n",
    "    articles = [] # Used to make a list of recipes\n",
    "    button = None # Used to interact with various button elements\n",
    "    container = None # Used to store various container elements\n",
    "    currentURL = \"\" # Used to store various urls \n",
    "    pages = [] # Used to append a list with pages links\n",
    "    recipeLinks = [] # Used to store recipe links\n",
    "    searchbar = None # Used to interact with search bar\n",
    "    source = \"\" # Used to get page source code\n",
    "    tag = None # Used to store various tag elements\n",
    "    title = \"\" # Used to get the title\n",
    "    totalPages = [] # Stores a list of pages\n",
    "\n",
    "    # File Management\n",
    "    count = 0 # Used in the creation of image filenames\n",
    "    parentDirectory = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/\" # Project directory\n",
    "    dataDirectory = \"\" # Used to create folder\n",
    "    imageDirectory = \"\" # Used to create folder \n",
    "    recipeDirectory = \"\" # Used to create modified folder names\n",
    "    imageFileName = \"\" # Used to create image files\n",
    "\n",
    "    # Scraped Information\n",
    "    recipeDetails = {} # Used to store all the scraped recipe details\n",
    "    imageScrapeLimiter = 0 # Used to limit the amount of times an image is scraped\n",
    "\n",
    "    allergens = \"\" # Used to store scraped allergens\n",
    "    alternatives = \"\" #Used to store scraped alternatives\n",
    "    description = \"\" # Used to store the scraped description of the recipe\n",
    "    freeFrom = \"\" # Used to store the scraped free from information\n",
    "    imageLinks = [] # Used to scrape all of a recipes image links\n",
    "    ingredients = \"\" # Used to store the scraped ingredients\n",
    "    instructions = \"\" # Used to store scraped instructions\n",
    "    mainPhoto = None # Used to store main photo link\n",
    "    name = \"\" # Used to store scraped recipe name\n",
    "    notes = \"\" # Used to store scraped recipe notes\n",
    "    recipeTags = \"\" # Used to store scraped recipe tags\n",
    "    storage = \"\" # Used to store scraped storage instructions\n",
    "    timeCook = \"\" # Used to store scraped cook time\n",
    "    timePrep = \"\" # Used to store scraped recipe  prep time \n",
    "    timeTotal = \"\" # Used to store scraped total time it takes to make the recipe\n",
    "\n",
    "class scraper:\n",
    "    '''\n",
    "    This class is the main scraper. \n",
    "\n",
    "    This class will navigate through a webpage and scrape data into a json file and images into a folder separated into recipes.\n",
    "\n",
    "    Attributes:\n",
    "    '''\n",
    "\n",
    "    def intitialize(self, url, searchTerm, delay):\n",
    "        '''\n",
    "        This function intitializes the scraper class.\n",
    "\n",
    "        Arg:\n",
    "            url (str): The target website\n",
    "            searchTerm (str): The word we want to type into the search bar\n",
    "            delay (int): The number of seconds used to delay a wait for elemnt and timeout exeptions\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "    \n",
    "        self.getURL(self, url) # Have to start somewhere\n",
    "        self.run(self)\n",
    "        self.closeSession() # Have to end somewhere\n",
    "    \n",
    "    # DECORATORS\n",
    "\n",
    "    def exceptionHandling(func):\n",
    "        @functools.wraps(func) # maintains introspection\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                func(*args, **kwargs)\n",
    "            except NoSuchElementException:\n",
    "                print(f\"{func.__name__} Exception: Element Not Found\")\n",
    "            except TimeoutException:\n",
    "                print(f\"{func.__name__} Exception: Timeout\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "\n",
    "    def functionTimer(func):\n",
    "        \"\"\"Prints the functions runtime\"\"\"\n",
    "        @functools.wraps(func) # maintains introspection\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.perf_counter()\n",
    "            func(*args, **kwargs)\n",
    "            end = time.perf_counter()\n",
    "            runtime = end - start\n",
    "            print (f\"Finished {func.__name__!r} in {runtime: .4f} secs\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "\n",
    "    # Best applied to small convenience functions that you don’t call directly yourself\n",
    "    def debug(func):\n",
    "        \"\"\"Print the function signature and return value\"\"\"\n",
    "        @functools.wraps(func) # maintains introspection\n",
    "        def wrapper(*args, **kwargs):\n",
    "            args_repr = [repr(a) for a in args]                     # Creates a list of positional arguments, repr() returns a string for each argument               \n",
    "            kwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()] # Creates a list of keyword arguments, f-string formats each argument as key=value, !r specifier means that repr() is used to represent the value\n",
    "            signature = \", \".join(args_repr + kwargs_repr)          # Join both lists together to make a signature\n",
    "            print(f\"Calling {func.__name__}({signature})\")\n",
    "            func(*args, **kwargs)\n",
    "            print(f\"{func.__name__!r} returned {func!r}\")           \n",
    "            return func\n",
    "        return wrapper\n",
    "\n",
    "    # Best applied to functions you only need to call less often\n",
    "    def slowDown(func):\n",
    "        '''Sleep before calling the function'''\n",
    "        @functools.wraps(func) # maintains introspection\n",
    "        def wrapper(*args, **kwargs):\n",
    "            time.sleep(1)\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "\n",
    "    def scrapeHandling(element):\n",
    "        '''Wraps all scrape functions with exception handling'''\n",
    "        def wrapperOuter(func):\n",
    "            @functools.wraps(func) # maintains introspection\n",
    "            def wrapperInner(*args, **kwargs):\n",
    "                try:\n",
    "                    func(*args, **kwargs)\n",
    "                except NoSuchElementException:\n",
    "                    #print(f\"{func.__name__} Exception:\", element, \"Not Found\")\n",
    "                    element = \"N/A\"\n",
    "                except TimeoutException:\n",
    "                    #print(f\"{func.__name__} Exception: Timeout\")\n",
    "                    element = \"N/A\"\n",
    "                return func\n",
    "            return wrapperInner\n",
    "        return wrapperOuter\n",
    "\n",
    "    def folderAlreadyExists(folderName) -> None:\n",
    "        '''Handels the folder already exists exeption in the folder creation functions'''\n",
    "        def wrapperOuter(func):\n",
    "            @functools.wraps(func) # maintains introspection\n",
    "            def wrapperInner(*args, **kwargs):\n",
    "                try:\n",
    "                    func(*args, **kwargs)\n",
    "                except:\n",
    "                    #print(f\"{func.__name__} Error\")\n",
    "                    #print(folderName, \"Folder Already Exists\")\n",
    "                    pass\n",
    "                return func\n",
    "            return wrapperInner\n",
    "        return wrapperOuter\n",
    "    \n",
    "    def callCount(func) -> None:\n",
    "        '''Counts the number of time the function has been called.'''\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            wrapper.calls += 1\n",
    "            print(f\"Call {wrapper.calls} of {func.__name__!r}\")\n",
    "            return func(*args, **kwargs)\n",
    "        wrapper.calls = 0\n",
    "        return wrapper\n",
    "\n",
    "    def run(self) -> None:\n",
    "        '''\n",
    "        This function calls all the necissary functions in order.\n",
    "\n",
    "        The order of function calls is for the class to navigate \n",
    "        through the website and scrape the text and images.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        #self.acceptCookies()\n",
    "        data.currentURL = self.findRecipeList(self)\n",
    "        self.getAllRecipePages(self, data.currentURL)\n",
    "        self.getRecipes(self, data.currentURL)\n",
    "        self.cycleRecipeLinks(self)\n",
    "        self.closeSession()   \n",
    "\n",
    "    def cycleRecipeLinks(self) -> None:\n",
    "        '''\n",
    "        This function iterates though the list of recipe urls, passing each url to the makeImage() function.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        for i in data.recipeLinks:\n",
    "            data.currentURL = i\n",
    "            self.makeImage(self, data.currentURL)\n",
    "\n",
    "    #@callCount\n",
    "    def getURL(self, url) -> None:\n",
    "        '''\n",
    "        Navigates to a website using a url passed as a perameter.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The target website\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        driver.get(url) \n",
    "\n",
    "    def getTitle() -> None:\n",
    "        '''\n",
    "        Fetches the title.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        data.title = driver.title\n",
    "\n",
    "    def closeSession() -> None:\n",
    "        '''\n",
    "        Closes the driver\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        driver.quit()\n",
    "\n",
    "    def getSourceCode() -> None:\n",
    "        '''\n",
    "        Fetches the current pages source code.\n",
    "        \n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.source = driver.page_source\n",
    "\n",
    "    @exceptionHandling\n",
    "    def search(self, searchTerm) -> None:\n",
    "        '''\n",
    "        Finds search bar and clicks it ready for input.\n",
    "        \n",
    "        Args:\n",
    "            searchTerm (str): The word passed to the findSearchBar() fucntion that types into the search box\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Find searchbar and click\n",
    "        button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'nav-searchbar-btn')))\n",
    "        button.click()\n",
    "\n",
    "        self.findSearchbar(self, searchTerm)\n",
    "\n",
    "    def searchbarTextAndClick(searchTerm) -> None:\n",
    "        '''\n",
    "        This function types the searchTerm into the searchbar and presses enter which then navigates to the search results page.\n",
    "\n",
    "        Args:\n",
    "            searchTerm (str): The word to type into the search box\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            data.searchbar.send_keys(searchTerm)\n",
    "            data.searchbar.send_keys(Keys.RETURN) # Return = Enter\n",
    "        except:\n",
    "            print(\"Exception: No search term input\")\n",
    "    \n",
    "    @exceptionHandling\n",
    "    def findSearchbar(self, searchTerm) -> None:\n",
    "        '''\n",
    "        This function finds the searchbar and calls the searchbarTextAndClick() function with the searchTerm parameter.\n",
    "\n",
    "        Args: \n",
    "            searchTerm (str): The word passed to the searchbarTextAndClick() fucntion that types into the search box\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        data.searchbar = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.NAME, \"sb\")))\n",
    "        self.searchbarTextAndClick(searchTerm)\n",
    "\n",
    "    @exceptionHandling\n",
    "    def home() -> None:\n",
    "        '''\n",
    "        Finds the title and clicks it.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        title = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'nav-image')))\n",
    "        title.click()\n",
    "\n",
    "    @exceptionHandling\n",
    "    def findRecipeList(self) -> None:\n",
    "        '''\n",
    "        Finds the recipe tab and clicks it.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        data.button = WebDriverWait(driver,5).until(EC.presence_of_element_located((By.LINK_TEXT, 'Recipes')))\n",
    "        data.button.click()\n",
    "\n",
    "    @exceptionHandling\n",
    "    def acceptCookies() -> None:\n",
    "        '''\n",
    "        Finds the accept cookies button and clicks it.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        data.button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/div[2]/div[2]')))\n",
    "        data.button.click()\n",
    "    \n",
    "    def getRecipes(self, url) -> None:\n",
    "        '''\n",
    "        Calls the functions to find the recipe container and puts all the recipes in a list.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The target website that contains all the recipe search results\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.getRecipeContainer()\n",
    "        self.makeRecipeList()\n",
    "\n",
    "    @exceptionHandling\n",
    "    def getRecipeContainer() -> None:\n",
    "        '''\n",
    "        This function finds the recipe container\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.container = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='index-item-container']/div/div[2]/ul\"))) \n",
    "\n",
    "    def makeRecipeList() -> None:\n",
    "        '''\n",
    "        This function finds the individual recipe page link and stores it in a list\n",
    "\n",
    "        This function finds an individual recipes link by identifying the container \n",
    "        with all the recipes, then loops through that contain to find the individual \n",
    "        recipes and takes the url for that recipe and stores it in a list.\n",
    "        \n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.articles = data.container.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "        for i in data.articles:\n",
    "            data.tag = i.find_element(By.TAG_NAME, 'a')\n",
    "            data.recipeLinks.append(data.tag.get_attribute('href'))\n",
    "\n",
    "    def getPageURL() -> None:\n",
    "        '''\n",
    "        Returns the current page url.\n",
    "        \n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.currentURL =  driver.current_url\n",
    "\n",
    "    def getAllRecipePages(self, url) -> None:\n",
    "        '''\n",
    "        Calls the functions to navigate to each recipe page by modifying the current url and stores them in a list.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The target website that contains all the recipe search results\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.getTotalPages()\n",
    "        self.getSearchList()\n",
    "\n",
    "    @exceptionHandling\n",
    "    def getTotalPages() -> None:\n",
    "        '''\n",
    "        This function counts the total number of page results.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        #totalPages = driver.find_element(By.CLASS_NAME, 'page-text') #actual\n",
    "        data.totalPages = [1, 2, 3] #temp to shorten runtime\n",
    "\n",
    "    def getSearchList() -> None:\n",
    "        '''\n",
    "        This function retrieves the url of each search result by modifying the url.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        for i in data.totalPages:\n",
    "            data.currentURL = driver.current_url\n",
    "            url_change = \"?page=\" + str(i)\n",
    "            next_page = data.currentURL + url_change\n",
    "            data.pages.append(next_page)\n",
    "\n",
    "    def getUniqueID(self, url) -> None:\n",
    "        '''\n",
    "        This function creates a uuid for each recipe by modifying  a url as a perameter. \n",
    "\n",
    "        Args: \n",
    "            url (str): The recipe page url\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        page_ID = url\n",
    "        just_ID = page_ID.replace(str(\"https://www.pickuplimes.com/recipe/\"), \"\")\n",
    "\n",
    "        ids = (just_ID, uuid.uuid4())\n",
    "\n",
    "        return ids\n",
    "\n",
    "    @scrapeHandling(data.name)\n",
    "    def scrapeName() -> None:\n",
    "        '''\n",
    "        This function scrapes the name of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.name = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/h1'))).text\n",
    "\n",
    "    @scrapeHandling(data.recipeTags)\n",
    "    def scrapeTags() -> None:\n",
    "        '''\n",
    "        This function scrapes the tags of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.recipeTags = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"header-info-col\"]/div/header/a[1]/div/p'))).text\n",
    "\n",
    "    @scrapeHandling(data.description)\n",
    "    def scrapeDescription() -> None:\n",
    "        '''\n",
    "        This function scrapes the description of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.description = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/span'))).text\n",
    "\n",
    "    @scrapeHandling(data.timeTotal)\n",
    "    def scrapeTotalTime() -> None:\n",
    "        '''\n",
    "        This function scrapes the total cook time of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.timeTotal = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[2]'))).text  \n",
    "\n",
    "    @scrapeHandling(data.timePrep)\n",
    "    def scrapePrepTime() -> None:\n",
    "        '''\n",
    "        This function scrapes the prep time of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.timePrep = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[3]'))).text\n",
    "\n",
    "    @scrapeHandling(data.timeCook)\n",
    "    def scrapeCookTime() -> None:\n",
    "        '''\n",
    "        This function scrapes the cook time of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.timeCook = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[4]'))).text\n",
    "\n",
    "    @scrapeHandling(data.allergens)\n",
    "    def scrapeAllergens() -> None:\n",
    "        '''\n",
    "        This function scrapes the allergens of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        data.allergens = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[1]/div'))).text \n",
    "\n",
    "    @scrapeHandling(data.alternatives)\n",
    "    def scrapeAlternatives() -> None:\n",
    "        '''\n",
    "        This function scrapes the alternative ingrediants of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.alternatives = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[2]/div'))).text\n",
    "\n",
    "    @scrapeHandling(data.freeFrom)\n",
    "    def scrapeFreeFrom() -> None:\n",
    "        '''\n",
    "        This function scrapes what the recipe is free from. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.freeFrom = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[3]/div'))).text \n",
    "\n",
    "    @scrapeHandling(data.ingredients)\n",
    "    def scrapeIngredients() -> None:\n",
    "        '''\n",
    "        This function scrapes the recipe ingredients. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.ingredients = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[2]'))).text\n",
    "\n",
    "    @scrapeHandling(data.instructions)\n",
    "    def scrapeInstructions() -> None:\n",
    "        '''\n",
    "        This function scrapes the instructions for the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.instructions = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ol'))).text\n",
    "\n",
    "    @scrapeHandling(data.notes)\n",
    "    def scrapeNotes() -> None:\n",
    "        '''\n",
    "        This function scrapes the notes from the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.notes = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[1]/li'))).text\n",
    "\n",
    "    @scrapeHandling(data.storage)\n",
    "    def scrapeStorage() -> None:\n",
    "        '''\n",
    "        This function scrapes the storage instructions of the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.storage = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[2]/li'))).text\n",
    "\n",
    "    @scrapeHandling(data.mainPhoto)\n",
    "    def scrapeMainPhoto() -> None:\n",
    "        '''\n",
    "        This function scrapes the main image from the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.mainPhoto = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main-image-container\"]/img')))\n",
    "\n",
    "\n",
    "    @exceptionHandling\n",
    "    def scrapeImages() -> None:\n",
    "        '''\n",
    "        This function scrapes the other images from the recipe. Exception handling is done with a decorator\n",
    "\n",
    "        This function finds the image container then puts its children into a list. A limit is set based on the number of images found for later use. Finally the list is \n",
    "        iterated to get each images url link to create a list of image links\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        imageContainer = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-video\"]/div[2]'))) # Find the container\n",
    "        imageList = imageContainer.find_elements(By.XPATH, 'img') # Find the children\n",
    "        data.imageScrapeLimiter = len(imageList)\n",
    "\n",
    "        for i in imageList:\n",
    "            link = i.get_attribute('src')\n",
    "            data.imageLinks.append(link)\n",
    "\n",
    "    @exceptionHandling\n",
    "    def getRecipeDetails(self, url) -> None:\n",
    "        '''\n",
    "        This function calls the scrape functions\n",
    "\n",
    "        This function navigates to a recipe page and calls all the scrape functions to collect \n",
    "        the data from the page. It the calls the function that stores all the data in a dictionary \n",
    "        and calls the function that writes that dictionary to a json file\n",
    "\n",
    "        Args:\n",
    "            url (str): The recipe page url\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        self.getURL(self, url)\n",
    "\n",
    "        self.scrapeName()  \n",
    "        self.scrapeTags()\n",
    "        self.scrapeDescription()\n",
    "        self.scrapeTotalTime()\n",
    "        self.scrapePrepTime()\n",
    "        self.scrapeCookTime()\n",
    "        self.scrapeAllergens()\n",
    "        self.scrapeAlternatives()\n",
    "        self.scrapeFreeFrom()\n",
    "        self.scrapeIngredients()\n",
    "        self.scrapeInstructions()\n",
    "        self.scrapeNotes()\n",
    "        self.scrapeStorage()\n",
    "        self.scrapeMainPhoto()\n",
    "        self.scrapeImages()\n",
    "\n",
    "        self.storeDetails(self, url)\n",
    "        self.jsonFile(self)\n",
    "\n",
    "    def storeDetails(self, url) -> None:\n",
    "        '''\n",
    "        This function updates the data dictionarl with all the scraped information\n",
    "\n",
    "        Args: \n",
    "            url (str): The recipe url to make a unique ID\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        data.recipeDetails = {'ID': [], 'Name': [], 'Photo': [],'Tags': [], 'Description': [], 'Total Time': [], 'Prep Time': [], 'Cook Time': [], 'Allergens': [], 'Swaps': [], 'Free From': [], 'Ingredients': [], 'Directions': [], 'Notes': [], 'Storage': [], 'Images': []}\n",
    "        data.recipeDetails['ID'].append(self.getUniqueID(self, url))\n",
    "        data.recipeDetails['Name'].append(data.name)\n",
    "        data.recipeDetails['Photo'].append(data.mainPhoto)\n",
    "        data.recipeDetails['Tags'].append(data.recipeTags)\n",
    "        data.recipeDetails['Description'].append(data.description)\n",
    "        data.recipeDetails['Total Time'].append(data.timeTotal)\n",
    "        data.recipeDetails['Prep Time'].append(data.timePrep)\n",
    "        data.recipeDetails['Cook Time'].append(data.timeCook)\n",
    "        data.recipeDetails['Allergens'].append(data.allergens)\n",
    "        data.recipeDetails['Swaps'].append(data.alternatives)\n",
    "        data.recipeDetails['Free From'].append(data.freeFrom)\n",
    "        data.recipeDetails['Ingredients'].append(data.ingredients)\n",
    "        data.recipeDetails['Directions'].append(data.instructions)\n",
    "        data.recipeDetails['Notes'].append(data.notes)\n",
    "        data.recipeDetails['Storage'].append(data.storage)\n",
    "        data.recipeDetails['Images'].append(data.imageLinks)\n",
    "\n",
    "    def jsonFile(self) -> None:\n",
    "        '''This function creates a folder for the json file to be stored in\n",
    "        \n",
    "        This function creates a folder called 'raw_data' in the path for the \n",
    "        json file to be saved in. Uses a try except catch as it will throw an \n",
    "        error if the folder already exists.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.makeRaw_DataFolder()\n",
    "\n",
    "        # Deals with TypeError: Object of type UUID is not JSON serializable by encoding the UUID\n",
    "        JSONEncoder_olddefault = JSONEncoder.default\n",
    "        def JSONEncoder_newdefault(self, o):\n",
    "            if isinstance(o, UUID): return str(o)\n",
    "            return JSONEncoder_olddefault(self, o)\n",
    "        JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "        self.jsonDump()\n",
    "\n",
    "    @folderAlreadyExists(\"raw_data\")\n",
    "    def makeRaw_DataFolder():\n",
    "        '''\n",
    "        This function creates a folder.\n",
    "        \n",
    "        This function creates a folder for the json files to be stored in\n",
    "        Throws an exception if the folder already exists which is handeled \n",
    "        with a decorator.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.dataDirectory = \"raw_data\"\n",
    "        parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "        path = os.path.join(parent_dir, data.dataDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.dataDirectory)\n",
    "    \n",
    "    def jsonDump() -> None:\n",
    "        '''This function writes the dictionary data to a json file\n",
    "        \n",
    "        This function stores data by writing the 'recipe_details' dictionary to a JSON file called 'data.json' in the folder just created\n",
    "        The dicrionary is converted to a string using str() to deal with 'TypeError: Object of type WebElement is not JSON serializable\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        with open(os.path.join('raw_data', 'data.json'), 'w') as json_file:\n",
    "            json.dump(str(data.recipeDetails), json_file)\n",
    "\n",
    "    def downloadImage(self, url) -> None:\n",
    "        '''\n",
    "        This function creates a folder.\n",
    "        \n",
    "        This function calls the function that creates a folder called 'images' \n",
    "        Then calls the function that creates a folder named after the recipes name \n",
    "        Adds User-Agent Headers in a try/catch exeption handler to bypass 403 error\n",
    "        Downloads the image into the folder of that recipe name\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        self.makeImagesFolder()\n",
    "        self.makeRecipeFolder()\n",
    "\n",
    "        try:\n",
    "            # Adds headers to resolve 403 Fobidden Error\n",
    "            opener=urllib.request.build_opener()\n",
    "            opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "            urllib.request.install_opener(opener)\n",
    "            path = os.path.join(data.recipeDirectory, data.imageFileName + '.jpg')\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        except:\n",
    "            print(\"Error Downloading Images\")         \n",
    "\n",
    "    @folderAlreadyExists(\"Images\")\n",
    "    def makeImagesFolder() -> None:\n",
    "        '''\n",
    "        This function makes a folder.\n",
    "\n",
    "        This function Makes a folder called images for the recipe images to be stored in.\n",
    "        Uses a try except catch in a decorator as it will throw an error if the folders already exists.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.imageDirectory = \"images\"\n",
    "        path = os.path.join(data.parentDirectory, data.imageDirectory)\n",
    "        os.mkdir(path)\n",
    "\n",
    "    @folderAlreadyExists(\"Recipe\")\n",
    "    def makeRecipeFolder() -> None:\n",
    "        '''\n",
    "        This function makes a folder.\n",
    "\n",
    "        This function Makes a folder named after the recipe for the images to be stored in.\n",
    "        Uses a try except catch in a decorator as it will throw an error if the folders already exists.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        \n",
    "        data.recipeDirectory = \"images/\" + data.name\n",
    "        path = os.path.join(data.parentDirectory, data.recipeDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.recipeDirectory)\n",
    "\n",
    "    def makeImage(self, url) -> None:\n",
    "        '''\n",
    "        This function makes a file name for the image and downloads it.\n",
    "        \n",
    "        This function retrieves the ID of each image using 'getRecipeDetails().\n",
    "        Removes all unecissary elements from the ID string to create a file name.\n",
    "        Pass the file name to 'downloadImages() to create a file.\n",
    "\n",
    "        Args:\n",
    "            url (str): The recipe url to scrape information from\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "\n",
    "        self.getRecipeDetails(self, url)\n",
    "\n",
    "        for i in data.recipeDetails['Images']:\n",
    "\n",
    "            for j in i:\n",
    "                data.imageFileName = data.name + \" \" + str(data.count) + \".jpg\"\n",
    "                self.downloadImage(self, j)\n",
    "\n",
    "                if data.count < data.imageScrapeLimiter:\n",
    "                    data.count = data.count + 1\n",
    "                else:\n",
    "                    data.count = 0\n",
    "\n",
    "scraper.intitialize(scraper, 'https://www.pickuplimes.com', 'lemons', 5)\n",
    "\n",
    "# IDEALS ---------------------------------------------------------------------------------------------------------------\n",
    "# link getTitle() and home()\n",
    "# link getPageURL() and getUniqueID()\n",
    "# get search results\n",
    "# get search results on more than the first page\n",
    "# make a separate main image\n",
    "# counts how many recipes there are on the site\n",
    "# a for loop that automaticaly changes the xpath to get all the images for the recipe\n",
    "# replace all XPaths with written XPaths\n",
    "# a method that makes a list containing all buttons and cycles through them looking for a link_text given as an argument\n",
    "# seperate website specific methods from general functioning in a class\n",
    "# make sure all code works with any website so is reusable and genralisable\n",
    "# use the unique id (page url) to stop scraping recipes it has already scraped \n",
    "# have a wait for element function that takes a xpath and a perameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".................."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_downloadImage\n",
      "test_getURL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......F....................."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website Found\n",
      "test_makeImage\n",
      "test_makeImagesFolder\n",
      "test_makeRaw_DataFolder\n",
      "test_makeRecipeFolder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAIL: test_makeRaw_DataFolder (__main__.scraperTestCase)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Millie\\AppData\\Local\\Temp\\ipykernel_220\\3239744656.py\", line 175, in test_makeRaw_DataFolder\n",
      "    self.assertEqual(self.dataPath, 'C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/raw_data')\n",
      "AssertionError: 'C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\\\\raw_data' != 'C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/raw_data'\n",
      "- C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\\raw_data\n",
      "?                                                               ^\n",
      "+ C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/raw_data\n",
      "?                                                               ^\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 46 tests in 1.866s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x159e5638f70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os # for testing directories\n",
    "import requests # for testing website response\n",
    "import unittest\n",
    "import uuid # for testing the recipe unique identifyer\n",
    "from scraper import scraper\n",
    "\n",
    "class scraperTestCase(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls): # Runs at the begining of the file\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls): # Runs at the end of the file\n",
    "        pass\n",
    "\n",
    "    def setUp(self): # Runs before every test\n",
    "        self.bot1 = scraper() \n",
    "        self.recipe = 'harissa-spiced-beans-898-0.jpg'\n",
    "        self.directory = 'C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline'\n",
    "        self.friendlyID = 'harissa-spiced-beans-898'\n",
    "        self.systemID = uuid.uuid4()\n",
    "        self.dictionary = {\"ID\": []}\n",
    "        self.number = str(3)\n",
    "        self.recipeName = 'Harissa Spiced Beans'\n",
    "\n",
    "        self.folderName1 = 'images'\n",
    "        self.folderName2 = 'raw_data'\n",
    "        self.imagePath = os.path.join(self.directory, self.folderName1)\n",
    "        self.recipePath = os.path.join(self.directory, self.folderName1)\n",
    "        self.dataPath = os.path.join(self.directory, self.folderName2)\n",
    "        #self.handle = open(\"data.json\", \"r\") # Dont forget to test the json file\n",
    "\n",
    "    def tearDown(self): # Runs at the end of every test\n",
    "        del self.bot1\n",
    "    \n",
    "    def test_dataClass(self):\n",
    "        pass\n",
    "    \n",
    "    def test_decoratorClass(self):\n",
    "        pass\n",
    "\n",
    "    def test_exceptionHandling(self):\n",
    "        pass\n",
    "\n",
    "    def test_run(self):\n",
    "        pass\n",
    "\n",
    "    def test_cycleRecipeLinks(self):\n",
    "        pass\n",
    "\n",
    "    def test_getURL(self):\n",
    "        print(\"test_getURL\")\n",
    "        response = requests.get('https://www.pickuplimes.com/')\n",
    "\n",
    "        if response.ok:\n",
    "            print(\"Website Found\")\n",
    "            return response.text\n",
    "        else:\n",
    "            print(\"!Website Not Found!\")\n",
    "            return 'Bad Response'\n",
    "\n",
    "    def test_getTitle(self):\n",
    "        pass\n",
    "\n",
    "    def test_closeSession(self):\n",
    "        pass\n",
    "\n",
    "    def test_getSourceCode(self):\n",
    "        pass\n",
    "\n",
    "    def test_search(self):\n",
    "        pass\n",
    "\n",
    "    def test_searchbarTextAndClick(self):\n",
    "        pass\n",
    "\n",
    "    def test_findSearchbar(self):\n",
    "        pass\n",
    "\n",
    "    def test_home(self):\n",
    "        pass\n",
    "\n",
    "    def test_findRecipeList(self):\n",
    "        pass\n",
    "\n",
    "    def test_acceptCookies(self):\n",
    "        pass\n",
    "\n",
    "    def test_getRecipes(self):\n",
    "        pass\n",
    "\n",
    "    def test_getRecipeContainer(self):\n",
    "        pass\n",
    "\n",
    "    def test_makeRecipeList(self):\n",
    "        pass\n",
    "\n",
    "    def test_getPageURL(self):\n",
    "        pass\n",
    "\n",
    "    def test_getAllRecipePages(self):\n",
    "        pass\n",
    "\n",
    "    def test_getTotalPages(self):\n",
    "        pass\n",
    "\n",
    "    def test_getSearchList(self):\n",
    "        pass\n",
    "\n",
    "    def test_getUniqueID(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeName(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeTags(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeDescription(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeTotalTime(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapePrepTime(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeCookTime(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeAllergens(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeAlternatives(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeFreeFrom(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeIngredients(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeInstructions(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeInstructions(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeNotes(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeStorage(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeMainPhoto(self):\n",
    "        pass\n",
    "\n",
    "    def test_scrapeImages(self):\n",
    "        pass\n",
    "\n",
    "    def test_getRecipeDetails(self):\n",
    "        pass\n",
    "\n",
    "    def test_storeDetails(self):\n",
    "        pass\n",
    "\n",
    "    def test_jsonFile(self):\n",
    "        pass\n",
    "\n",
    "    def test_makeRaw_DataFolder(self):\n",
    "        print('test_makeRaw_DataFolder')\n",
    "\n",
    "        # Check directory path \n",
    "        self.assertEqual(self.dataPath, 'C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/raw_data')\n",
    "\n",
    "        # Check file name\n",
    "        self.assertEqual(os.path.join(self.recipePath, self.recipe), '')\n",
    "\n",
    "        data.dataDirectory = \"raw_data\"\n",
    "        \n",
    "        '''\n",
    "        path = os.path.join(parent_dir, data.dataDirectory)\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % data.dataDirectory)\n",
    "        '''\n",
    "\n",
    "        # UNFINISHED\n",
    "    def jsonDump(self):\n",
    "        print('jsonDump')\n",
    "\n",
    "        # Check json file was created\n",
    "\n",
    "        # Check dictionary was written in json\n",
    "    \n",
    "    # UNFINISHED\n",
    "    def test_downloadImage(self):\n",
    "        print('test_downloadImage')\n",
    "\n",
    "        # Check directory path\n",
    "        self.assertEqual(self.recipePath, 'C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\\images')\n",
    "\n",
    "        # Check image name\n",
    "        self.assertEqual(self.recipeName, 'Harissa Spiced Beans') \n",
    "\n",
    "        # Check image was downloaded\n",
    "\n",
    "    # UNFINISHED\n",
    "    def test_makeImagesFolder(self):\n",
    "        print('test_makeImagesFolder')       \n",
    "\n",
    "        # Check directory is correct\n",
    "        self.assertEqual(self.imagePath, 'C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\\images') \n",
    "\n",
    "        # Check folder was created\n",
    "\n",
    "    def test_makeRecipeFolder(self):\n",
    "        print('test_makeRecipeFolder')\n",
    "\n",
    "        # Check edited friendlyID\n",
    "        self.assertEqual(self.recipe.replace(\".jpg\", \"\").replace(\"0\", \"\"), 'harissa-spiced-beans-898-')\n",
    "\n",
    "        # Check directory\n",
    "        self.assertEqual(self.recipePath, 'C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\\images')\n",
    "\n",
    "    def test_makeImage(self):\n",
    "        print(\"test_makeImage\")\n",
    "\n",
    "        self.dictionary['ID'].append(self.friendlyID)\n",
    "        self.dictionary['ID'].append(self.systemID)\n",
    "\n",
    "        # Check the dictionary contains the correct IDs\n",
    "        self.assertEqual(self.dictionary['ID'][0], self.friendlyID)\n",
    "        self.assertAlmostEqual(self.dictionary['ID'][1], self.systemID)\n",
    "\n",
    "        # Check it removes all unnecessary bits from the string\n",
    "        editedID = (self.dictionary['ID'][0])\n",
    "        editedID = editedID.title()\n",
    "        editedID = editedID.replace(\"-\", \" \")\n",
    "        for i in editedID:\n",
    "            if i.isdigit():\n",
    "                editedID = editedID.replace(i , \"\")\n",
    "\n",
    "        # Check the finished string is correct\n",
    "        name = editedID + self.number + \".jpg\"\n",
    "        self.assertEqual(name, editedID + self.number + \".jpg\")     \n",
    "\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    test_scraper() \n",
    "#    print(\"Passed\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89f331584aba9fd139438438d44e9f0981e5b7ec53d275c2a7e5fdd51f9e359f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataCollectionPipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
