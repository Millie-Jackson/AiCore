{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'scraper' has no attribute 'recipe_details'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 275>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=271'>272</a>\u001b[0m         \u001b[39mprint\u001b[39m(recipeData)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=272'>273</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownloadImage(url, recipeName)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=274'>275</a>\u001b[0m scraper\u001b[39m.\u001b[39;49mintitialize(scraper, \u001b[39m'\u001b[39;49m\u001b[39mhttps://www.pickuplimes.com\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlemons\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Millie\\Documents\\AiCore\\AiCore\\DataCollectionPipeline\\DataCollectionPipeline.ipynb Cell 1'\u001b[0m in \u001b[0;36mscraper.intitialize\u001b[1;34m(self, url, search_term, delay)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=28'>29</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetURL(url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=29'>30</a>\u001b[0m \u001b[39m#self.getTitle()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=30'>31</a>\u001b[0m \u001b[39m#self.acceptCookies()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=31'>32</a>\u001b[0m \u001b[39m#self.getAllRecipePages()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=41'>42</a>\u001b[0m \u001b[39m#self.jsonFile()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=42'>43</a>\u001b[0m \u001b[39m#self.downloadImage('/html/body/img', 'nameForImage')\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=43'>44</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetImages(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttps://www.pickuplimes.com/recipe/harissa-spiced-beans-898\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHarissa Spiced Beans\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecipe_details[\u001b[39m'\u001b[39m\u001b[39mImages\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=44'>45</a>\u001b[0m \u001b[39m#time.sleep(3)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline/DataCollectionPipeline.ipynb#ch0000000?line=46'>47</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcloseSession()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'scraper' has no attribute 'recipe_details'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "\n",
    "from uuid import UUID\n",
    "from json import JSONEncoder\n",
    "from urllib.request import Request, urlopen\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "class scraper:\n",
    "    def intitialize(self, url, search_term, delay):\n",
    "        global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "        #seconds = delay\n",
    "        \n",
    "        self.getURL(url)\n",
    "        #self.getTitle()\n",
    "        #self.acceptCookies()\n",
    "        #self.getAllRecipePages()\n",
    "        #self.getSourceCode()\n",
    "        #self.search(search_term)\n",
    "        #time.sleep(3)\n",
    "        #self.home()\n",
    "        #self.findRecipeList()\n",
    "        #self.getRecipes()\n",
    "        #self.getPageURL()\n",
    "        #self.getUniqueID()\n",
    "        #self.getRecipeDetails(self)\n",
    "        #self.jsonFile()\n",
    "        #self.downloadImage('/html/body/img', 'nameForImage')\n",
    "        self.getImages(self, 'https://www.pickuplimes.com/recipe/harissa-spiced-beans-898', 'Harissa Spiced Beans', self.recipe_details['Images'])\n",
    "        #time.sleep(3)\n",
    "\n",
    "        self.closeSession()\n",
    "\n",
    "    def getURL(url):\n",
    "        '''Navigates to a website using a url passed as a perameter.'''\n",
    "        driver.get(url) \n",
    "\n",
    "    def getTitle():\n",
    "        '''Fetches the title and prints it to screen.'''\n",
    "        print(driver.title)\n",
    "\n",
    "    def closeSession():\n",
    "        '''Closes the driver after 3 seconds.'''\n",
    "        time.sleep(3)\n",
    "        driver.quit()\n",
    "\n",
    "    def getSourceCode():\n",
    "        '''Fetches source code for the page.'''\n",
    "        print(driver.page_source)\n",
    "\n",
    "    def search(search_term):\n",
    "        '''Finds search bar, types in the search term whoch it takes as a perameter and clicks to navigate to the next page.'''\n",
    "        try:\n",
    "            button = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.ID, 'nav-searchbar-btn')))\n",
    "            button.click()\n",
    "            try:\n",
    "                search_bar = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.NAME, \"sb\")))\n",
    "                try:\n",
    "                    search_bar.send_keys(search_term)\n",
    "                    search_bar.send_keys(Keys.RETURN) # Return = Enter\n",
    "                except:\n",
    "                    print(\"Exception: No search term input\")\n",
    "            except NoSuchElementException:\n",
    "                print(\"Exception: No search bar found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Search bar\")\n",
    "\n",
    "    def home():\n",
    "        '''Finds the title and clicks it.'''\n",
    "        try:\n",
    "            title = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.ID, 'nav-image')))\n",
    "            title.click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: Title Not Found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Title\")\n",
    "\n",
    "    def findRecipeList():\n",
    "        '''Finds the recipe tab and clicks it.'''\n",
    "        try:\n",
    "            button = WebDriverWait(driver,seconds).until(EC.presence_of_element_located((By.LINK_TEXT, 'Recipes')))\n",
    "            button.click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: Recipe List Not Found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Recipe List\")\n",
    "\n",
    "    def acceptCookies():\n",
    "        '''Finds the accept cookies button and clicks it.'''\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/div[2]/div[2]')))\n",
    "            cookie_button.click()\n",
    "            print(\"Removed Cookies\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exeption: Didnt Find Cookie Button\")\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout: Accept Cookie\")\n",
    "    \n",
    "    def getRecipes():\n",
    "        '''Finds the recipe container and puts all the recipes in a list.'''\n",
    "        try:\n",
    "            main = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.ID, 'index-item-container')))      \n",
    "            articles = []\n",
    "            articles = main.find_elements(By.TAG_NAME, 'li')\n",
    "            print(\"Number of Recipes:\", len(articles))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: Can't Find Recipe List\")\n",
    "        except TimeoutException: \n",
    "            print(\"Exception: Timeout: Can't Find Recipe List\")\n",
    "\n",
    "        for i in articles:\n",
    "            print(\"Recipe:\" , i.text)\n",
    "\n",
    "    def getPageURL():\n",
    "        '''Prints the current page url.'''\n",
    "        print(\"URL:\", driver.current_url)\n",
    "\n",
    "    def getAllRecipePages(self):\n",
    "        '''Navigates to each recipe page and by modifying the current url and stores them in a list.'''\n",
    "        pages = []\n",
    "        self.getURL('https://www.pickuplimes.com/')\n",
    "        scraper.findRecipeList()\n",
    "        page = [driver.current_url]\n",
    "\n",
    "        try:\n",
    "            #total_pages = driver.find_element(By.CLASS_NAME, 'page-text') #actual\n",
    "            total_pages = [1, 2, 3, 4, 5] #temp to shorten runtime\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: Total Pages Not Found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Couldn't Find Total Pages\")\n",
    "\n",
    "        for i in total_pages:\n",
    "            current_page = driver.current_url\n",
    "            url_change = \"?page=\" + str(i)\n",
    "            next_page = current_page + url_change\n",
    "            print(next_page)\n",
    "            pages.append(next_page)\n",
    "            print(\"Number of Pages:\", len(pages))\n",
    "\n",
    "    def getUniqueID(self, url):\n",
    "        '''Creates a uuid for each recipe taking a url as a perameter'''\n",
    "        \n",
    "        page_ID = url\n",
    "        just_ID = page_ID.replace(str(\"https://www.pickuplimes.com/recipe/\"), \"\")\n",
    "\n",
    "        ids = (just_ID, uuid.uuid4())\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def getRecipeDetails(self):\n",
    "        self.getURL('https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "\n",
    "        try:\n",
    "            name = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/h1'))).text\n",
    "            tag = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"header-info-col\"]/div/header/a[1]/div/p'))).text\n",
    "            description = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"header-info-col\"]/div/header/span'))).text\n",
    "            time_total = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[2]'))).text  \n",
    "            time_prep = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[3]'))).text\n",
    "            time_cook = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-info-container\"]/div[4]'))).text\n",
    "            allergens = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[1]/div'))).text \n",
    "            swap = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[2]/div'))).text\n",
    "            free_from = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"allergen-info-container\"]/div[3]/div'))).text  \n",
    "            ingredients = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[2]'))).text\n",
    "            directions = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ol'))).text\n",
    "            notes = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[1]/li'))).text\n",
    "            storage = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[2]/li'))).text\n",
    "            picture_main = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main-image-container\"]/img')))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Exception: One Or More Data Entry Not Found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Exception: Timeout: Didnt Find All Data Entries\")\n",
    "\n",
    "        image_container = WebDriverWait(driver, seconds).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"recipe-video\"]/div[2]'))) # Find the container\n",
    "        image_list = image_container.find_elements(By.XPATH, 'img') # Find the children\n",
    "        print(len(image_list))\n",
    "        image_links= []\n",
    "\n",
    "        for i in image_list:\n",
    "            link = i.get_attribute('src')\n",
    "            image_links.append(link)\n",
    "        \n",
    "        self.recipe_details = {'ID': [], 'Name': [], 'Photo': [],'Tags': [], 'Description': [], 'Total Time': [], 'Prep Time': [], 'Cook Time': [], 'Allergens': [], 'Swaps': [], 'Free From': [], 'Ingredients': [], 'Directions': [], 'Notes': [], 'Storage': [], 'Images': []}\n",
    "        self.recipe_details['ID'].append(self.getUniqueID(self, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213'))\n",
    "        self.recipe_details['Name'].append(name)\n",
    "        self.recipe_details['Photo'].append(picture_main)\n",
    "        self.recipe_details['Tags'].append(tag)\n",
    "        self.recipe_details['Description'].append(description)\n",
    "        self.recipe_details['Total Time'].append(time_total)\n",
    "        self.recipe_details['Prep Time'].append(time_prep)\n",
    "        self.recipe_details['Cook Time'].append(time_cook)\n",
    "        self.recipe_details['Allergens'].append(allergens)\n",
    "        self.recipe_details['Swaps'].append(swap)\n",
    "        self.recipe_details['Free From'].append(free_from)\n",
    "        self.recipe_details['Ingredients'].append(ingredients)\n",
    "        self.recipe_details['Directions'].append(directions)\n",
    "        self.recipe_details['Notes'].append(notes)\n",
    "        self.recipe_details['Storage'].append(storage)\n",
    "        self.recipe_details['Images'].append(image_list)\n",
    "\n",
    "        self.jsonFile(self)\n",
    "        return self.recipe_details\n",
    "\n",
    "    def jsonFile(self):\n",
    "        '''Creates a folder called 'raw_data' in the path for the json file to be saved in\n",
    "        Uses a try except catch as it will throw an error if the folder already exists'''\n",
    "        try:\n",
    "            directory = \"raw_data\"\n",
    "            parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "            path = os.path.join(parent_dir, directory)\n",
    "            os.mkdir(path)\n",
    "            print(\"Directory '% s' created\" % directory)\n",
    "        except:\n",
    "            print(\"Root Folder raw_data Already Exists\")\n",
    "\n",
    "        '''Deals with TypeError: Object of type UUID is not JSON serializable by encoding the UUID'''\n",
    "        JSONEncoder_olddefault = JSONEncoder.default\n",
    "        def JSONEncoder_newdefault(self, o):\n",
    "            if isinstance(o, UUID): return str(o)\n",
    "            return JSONEncoder_olddefault(self, o)\n",
    "        JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "        '''Stores data by writing the 'recipe_details' dictionary to a JSON file called 'data.json' in the folder just created\n",
    "        The dicrionary is converted to a string using str() to deal with 'TypeError: Object of type WebElement is not JSON serializable'''\n",
    "        with open(os.path.join('raw_data', 'data.json'), 'w') as json_file:\n",
    "            json.dump(str(self.recipe_details), json_file)\n",
    "\n",
    "    def downloadImage(url, recipeName):\n",
    "        '''Creates a folder called 'images' in the path for the image files to be saved in\n",
    "        Uses a try except catch as it will throw an error if the folder already exists\n",
    "        Adds User-Agent Headers to bypass 403 error\n",
    "        Downloads an image into the images folder'''\n",
    "        try:\n",
    "            directory = \"images\"\n",
    "            parent_dir = \"C:/Users/Millie/Documents/AiCore/AiCore/DataCollectionPipeline\"\n",
    "            path = os.path.join(parent_dir, directory)\n",
    "            os.mkdir(path)\n",
    "            print(\"Directory '% s' created\" % directory)\n",
    "        except:\n",
    "            print(\"Root Folder 'images' Already Exists\")\n",
    "        \n",
    "        try:\n",
    "            # Adds headers to resolve 403 Fobidden Error\n",
    "            opener=urllib.request.build_opener()\n",
    "            opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "            urllib.request.install_opener(opener)\n",
    "\n",
    "            image = urllib.request.urlretrieve('https://cdn.pickuplimes.com/cache/29/ae/29ae9ff86850e9955f57a7ab8d5d29db.jpg', 'images/recipeName.jpg')\n",
    "            testImage = urllib.request.urlretrieve('https://cdn.vox-cdn.com/thumbor/jGJ8H_Z4JPl5CyDY-cIWpkhzELw=/0x0:2040x1360/920x613/filters:focal(857x517:1183x843):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/68829483/acastro_210104_1777_google_0001.0.jpg', 'images/test.jpg')\n",
    "        except:\n",
    "            print(\"Error Downloading Image\")\n",
    "\n",
    "\n",
    "\n",
    "scraper.intitialize(scraper, 'https://www.pickuplimes.com', 'lemons', 5)\n",
    "#global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "#print(global_ids)\n",
    "\n",
    "# IDEALS\n",
    "# store all recipe images in a list\n",
    "# link getTitle() and home()\n",
    "# link getPageURL() and getUniqueID()\n",
    "# get search results\n",
    "# get search results on more than the first page\n",
    "# counts how many recipes there are\n",
    "# a for loop that automaticaly changes the xpath to get all the images for the recipe\n",
    "# bypass login\n",
    "# replace all XPaths with written XPatha\n",
    "# a method that makes a list containing all buttons and cycles through them looking for a link_text given as an argument\n",
    "# seperate website specific methods from general functioning\n",
    "# make sure all code works with any website so is reusable and genralisable"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4aa49906c49aab2699de75d29353ef6218c1548a647ded36ad1d3c83fbb1a8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataCollectionPipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
