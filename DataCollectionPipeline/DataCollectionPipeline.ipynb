{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spicy-garlic-wok-noodles-213', UUID('edf29886-dc92-4fd5-b8a4-296d5fb481a2'))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "class scraper:\n",
    "    def intitialize(self, url, search_term):\n",
    "        global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "        self.getURL(url)\n",
    "        #self.getTitle()\n",
    "        #self.acceptCookies()\n",
    "        #self.getAllRecipePages()\n",
    "        #self.getSourceCode()\n",
    "        #self.search(search_term)\n",
    "        #time.sleep(3)\n",
    "        #self.home()\n",
    "        #self.findRecipeList()\n",
    "        #self.getRecipeDetails()\n",
    "        #self.getRecipes()\n",
    "        #self.getPageURL()\n",
    "        #self.getUniqueID()\n",
    "        #self.getRecipeDetails()\n",
    "        #self.getRecipeDetails(self)\n",
    "        #time.sleep(3)\n",
    "        self.closeSession()\n",
    "\n",
    "    def getURL(url):\n",
    "        driver.get(url) \n",
    "\n",
    "    def getTitle():\n",
    "        print(driver.title)\n",
    "\n",
    "    def closeSession():\n",
    "        time.sleep(3)\n",
    "        driver.quit()\n",
    "\n",
    "    def getSourceCode():\n",
    "        print(driver.page_source)\n",
    "    \n",
    "    def search(search_term):\n",
    "        try:\n",
    "            button = driver.find_element(By.ID, 'nav-searchbar-btn')\n",
    "            button.click()\n",
    "            try:\n",
    "                search_bar = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, \"sb\")))\n",
    "                try:\n",
    "                    search_bar.send_keys(search_term)\n",
    "                    search_bar.send_keys(Keys.RETURN) # Return = Enter\n",
    "                except:\n",
    "                    print(\"Exception: No search term input\")\n",
    "            except:\n",
    "                print(\"Exception: No search bar found\")\n",
    "        except:\n",
    "            print(\"Exception: No search button found\")\n",
    "\n",
    "    def home():\n",
    "        title = driver.find_element(By.ID, 'nav-image')\n",
    "        title.click()\n",
    "\n",
    "    def findRecipeList():\n",
    "        button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.LINK_TEXT, 'Recipes')))\n",
    "        button.click()\n",
    "\n",
    "    def acceptCookies():\n",
    "        try:\n",
    "            cookie_button = driver.find_element(By.XPATH, '/html/body/div/div[2]/div[2]')\n",
    "            cookie_button.click()\n",
    "            print(\"Removed Cookies\")\n",
    "        except:\n",
    "            print(\"Exeption: Didnt Find Cookie Button\")\n",
    "    \n",
    "    def getRecipes():\n",
    "        main = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'index-item-container')))      \n",
    "        print(\"Found results\")\n",
    "        articles = main.find_elements(By.TAG_NAME, 'li')\n",
    "        print(\"Number of Recipes:\", len(articles))\n",
    "\n",
    "        for i in articles:\n",
    "            print(\"Recipe:\" , i.text)\n",
    "\n",
    "    def getPageURL():\n",
    "        print(\"URL:\", driver.current_url)\n",
    "\n",
    "    def getAllRecipePages(self):\n",
    "        pages = []\n",
    "        self.getURL('https://www.pickuplimes.com/')\n",
    "        scraper.findRecipeList()\n",
    "        page = [driver.current_url]\n",
    "\n",
    "        #total_pages = driver.find_element(By.CLASS_NAME, 'page-text')\n",
    "\n",
    "        total_pages = [1, 2, 3, 4, 5]\n",
    "\n",
    "        for i in total_pages:\n",
    "            current_page = driver.current_url\n",
    "            url_change = \"?page=\" + str(i)\n",
    "            next_page = current_page + url_change\n",
    "            print(next_page)\n",
    "            pages.append(next_page)\n",
    "            print(\"Number of Pages:\", len(pages))\n",
    "\n",
    "    def getUniqueID(self, url):\n",
    "        \n",
    "        page_ID = url\n",
    "        just_ID = page_ID.replace(str(\"https://www.pickuplimes.com/recipe/\"), \"\")\n",
    "\n",
    "        ids = (just_ID, uuid.uuid4())\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def getRecipeDetails(self):\n",
    "        self.getURL('https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "\n",
    "        name = driver.find_element(By.XPATH, '//*[@id=\"header-info-col\"]/div/header/h1')\n",
    "        tag = driver.find_element(By.XPATH, '//*[@id=\"header-info-col\"]/div/header/a[1]/div/p')\n",
    "        description = driver.find_element(By.XPATH, '//*[@id=\"header-info-col\"]/div/header/span') \n",
    "        time_total = driver.find_element(By.XPATH, '//*[@id=\"recipe-info-container\"]/div[2]')\n",
    "        time_prep = driver.find_element(By.XPATH, '//*[@id=\"recipe-info-container\"]/div[3]')\n",
    "        time_cook = driver.find_element(By.XPATH, '//*[@id=\"recipe-info-container\"]/div[4]')\n",
    "        allergens = driver.find_element(By.XPATH, '//*[@id=\"allergen-info-container\"]/div[1]/div') \n",
    "        swap = driver.find_element(By.XPATH, '//*[@id=\"allergen-info-container\"]/div[2]/div')\n",
    "        free_from = driver.find_element(By.XPATH, '//*[@id=\"allergen-info-container\"]/div[3]/div')  \n",
    "        ingredients = driver.find_element(By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[2]')\n",
    "        directions = driver.find_element(By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ol')\n",
    "        notes = driver.find_element(By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[1]/li')\n",
    "        storage = driver.find_element(By.XPATH, '//*[@id=\"ingredient-direction-container\"]/div/div[4]/section/ul[2]/li')\n",
    "        nutrition = driver.find_element(By.XPATH, '//*[@id=\"nut-info\"]')\n",
    "\n",
    "        picture_main = driver.find_element(By.XPATH, '//*[@id=\"main-image-container\"]/img')\n",
    "\n",
    "        #print(\"Recipe:\", name.text)\n",
    "        #print(\"Type:\", tag.text)\n",
    "        #print(\"Description:\", description.text)\n",
    "        #print(time_total.text)\n",
    "        #print(time_prep.text)\n",
    "        #print(time_cook.text)\n",
    "        #print(\"Allergens:\", allergens.text)\n",
    "        #print(\"Swap:\", swap.text)\n",
    "        #print(\"Free from:\", free_from.text)\n",
    "        #print(ingredients.text)\n",
    "        #print(directions.text)\n",
    "        #print(\"Notes:\", notes.text)\n",
    "        #print(\"Storage:\", storage.text)\n",
    "\n",
    "        self.closeSession()\n",
    "\n",
    "scraper.intitialize(scraper, 'https://www.pickuplimes.com', 'lemons')\n",
    "#global_ids = scraper.getUniqueID(scraper, 'https://www.pickuplimes.com/recipe/spicy-garlic-wok-noodles-213')\n",
    "#print(global_ids)\n",
    "#scraper.closeSession()\n",
    "\n",
    "# IDEALS\n",
    "# store all recipe images in a list\n",
    "# link getTitle() and home()\n",
    "# link getPageURL() and getUniqueID()\n",
    "# get search results\n",
    "# get search results on more than the first page\n",
    "# counts how many recipes there are\n",
    "# gets recipe info and puts it in a dictionary\n",
    "# bypass login\n",
    "# store all recipe unique id in a list\n",
    "# replace all XPaths\n",
    "# a method that makes a list containing all buttons and cycles through them looking for a link_text given as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to frame\n",
      "Accepted Cookies\n",
      "https://www.zoopla.co.uk/for-sale/details/61207984/?search_identifier=c14ac08bf282507f6273830c65fcde12\n",
      "{'Price': ['Â£475,000'], 'Address': ['Langdale Road, Thornton Heath, Surrey CR7'], 'Bedrooms': ['3 beds'], 'Description': ['This mid-terrace house, which is in need of some modernisation, would make an ideal family home.\\n\\nThe ground floor comprises two reception rooms, kitchen, and a lean too with access to the private rear garden from both the kitchen and the rear reception.\\n\\nUpstairs has two double bedrooms and a good-sized single bedroom as well as a second bathroom with three-piece suite.\\n\\nThe property is situated within walking distance to local amenities and schools as well as Thornton Heath Station.']}\n"
     ]
    }
   ],
   "source": [
    "# ZOOPLA\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "url = \"https://zoopla.co.uk\" \n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5) # Slow it down otherwise it will miss the button \n",
    "\n",
    "try: \n",
    "    frame_id = \"gdpr-consent-notice\"\n",
    "    driver.switch_to.frame(frame_id)\n",
    "    print(\"Switched to frame\")\n",
    "    cookie_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
    "    cookie_button.click()\n",
    "    print(\"Accepted Cookies\")\n",
    "except:\n",
    "    print(\"No Cookie Button\")\n",
    "    pass\n",
    "\n",
    "url = \"https://www.zoopla.co.uk/for-sale/property/london/?q=london&results_sort=newest_listings&search_source=home\"\n",
    "driver.get(url)\n",
    "\n",
    "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61207984\"]/div[1]/div[2]')\n",
    "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
    "link = a_tag.get_attribute('href')\n",
    "driver.get(link)\n",
    "print(link)\n",
    "\n",
    "price = driver.find_element(By.XPATH, '//span[@data-testid=\"price\"]').text \n",
    "address = driver.find_element(By.XPATH, '//span[@data-testid=\"address-label\"]').text \n",
    "bedrooms = driver.find_element(By.XPATH, '//span[@data-testid=\"beds-label\"]').text  \n",
    "\n",
    "div_tag = driver.find_element(By.XPATH, '//div[@data-testid=\"truncated_text_container\"]') \n",
    "span_tag = div_tag.find_element(By.XPATH,'.//span') \n",
    "description = span_tag.text \n",
    "\n",
    "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []} \n",
    "dict_properties['Price'].append(price) \n",
    "dict_properties['Address'].append(address) \n",
    "dict_properties['Bedrooms'].append(bedrooms) \n",
    "dict_properties['Description'].append(description) \n",
    "dict_properties\n",
    "print(dict_properties)\n",
    "\n",
    "time.sleep(3)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1, 2, 3]\n",
    "print(my_list * 2)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4aa49906c49aab2699de75d29353ef6218c1548a647ded36ad1d3c83fbb1a8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataCollectionPipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
